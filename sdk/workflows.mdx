---
title: "Complex Workflows"
description: "Build sophisticated multi-step workflows with conditional logic, error handling, and parallel processing"
---

# Complex Workflows

Learn how to build sophisticated agent workflows with multiple steps, conditional execution, error handling, and parallel processing patterns.

## Workflow Fundamentals

### Sequential Workflows

Basic step-by-step execution with dependencies:

```python
from erdo import Agent, Step
from erdo.actions import llm, codeexec, memory

research_agent = Agent(
    name="research_workflow",
    description="Multi-step research and analysis workflow"
)

# Step 1: Research topic
research_step = Step(
    agent=research_agent,
    key="research",
    actiontype=llm.message(
        model="claude-sonnet-4",
        query="Research {{topic}} and gather key information",
        tools=[web_search_tool, web_parser_tool]
    )
)

# Step 2: Analyze findings (depends on research)
analysis_step = Step(
    agent=research_agent,
    key="analyze",
    actiontype=llm.message(
        model="claude-sonnet-4",
        query="Analyze research findings: {{steps.research.content}}",
        system_prompt="You are an expert analyst. Provide structured insights."
    ),
    depends_on=[research_step]
)

# Step 3: Generate report (depends on analysis)
report_step = Step(
    agent=research_agent,
    key="report",
    actiontype=llm.message(
        model="claude-sonnet-4",
        query="Create comprehensive report from analysis: {{steps.analyze.content}}"
    ),
    depends_on=[analysis_step]
)

# Step 4: Store insights
memory_step = Step(
    agent=research_agent,
    key="store",
    actiontype=memory.store(
        memory={
            "content": "{{steps.report.content}}",
            "description": "Research report on {{topic}}",
            "tags": ["research", "{{topic}}", "report"]
        }
    ),
    depends_on=[report_step]
)
```

### Parallel Processing

Execute multiple steps simultaneously for efficiency:

```python
from erdo import ExecutionMode

# Create parallel data processing workflow
data_processor = Agent(
    name="parallel_processor",
    description="Processes multiple data sources in parallel"
)

# These steps can run in parallel
process_sales_step = Step(
    agent=data_processor,
    key="process_sales",
    actiontype=codeexec.execute(
        code="process_sales_data({{sales_data}})"
    )
)

process_marketing_step = Step(
    agent=data_processor,
    key="process_marketing",
    actiontype=codeexec.execute(
        code="process_marketing_data({{marketing_data}})"
    )
)

process_customer_step = Step(
    agent=data_processor,
    key="process_customer",
    actiontype=codeexec.execute(
        code="process_customer_data({{customer_data}})"
    )
)

# Aggregate results after parallel processing
aggregate_step = Step(
    agent=data_processor,
    key="aggregate",
    actiontype=codeexec.execute(
        code="""
# Combine results from parallel processing
sales_results = {{steps.process_sales.output}}
marketing_results = {{steps.process_marketing.output}}
customer_results = {{steps.process_customer.output}}

combined_insights = aggregate_data_insights(
    sales_results, marketing_results, customer_results
)
generate_executive_summary(combined_insights)
"""
    ),
    depends_on=[process_sales_step, process_marketing_step, process_customer_step]
)
```

## Conditional Execution

### Condition-Based Workflows

Execute steps based on dynamic conditions:

```python
from erdo._generated.condition import (
    And, Or, Not, IsSuccess, IsError,
    TextContains, GreaterThan, LessThan
)

# Smart processing workflow
smart_processor = Agent(
    name="smart_processor",
    description="Adapts processing based on data characteristics"
)

# Initial data analysis
analyze_data_step = Step(
    agent=smart_processor,
    key="analyze_data",
    actiontype=llm.message(
        model="claude-sonnet-4",
        query="Analyze data characteristics: {{input_data}}",
        response_format={
            "Type": "json_schema",
            "Schema": {
                "schema": {
                    "type": "object",
                    "properties": {
                        "data_size": {"type": "number"},
                        "data_quality": {"type": "string"},
                        "complexity": {"type": "string"},
                        "processing_recommendation": {"type": "string"}
                    }
                }
            }
        }
    )
)

# Simple processing for small datasets
simple_processing = Step(
    agent=smart_processor,
    key="simple_process",
    actiontype=codeexec.execute(
        code="simple_data_processing({{input_data}})"
    ),
    depends_on=[analyze_data_step],
    execution_mode=ExecutionMode(
        mode="all",
        if_condition=And(
            IsSuccess(step="analyze_data"),
            LessThan(number="{{steps.analyze_data.data_size}}", value="1000"),
            TextContains(text="{{steps.analyze_data.complexity}}", value="low")
        )
    )
)

# Advanced processing for large or complex datasets
advanced_processing = Step(
    agent=smart_processor,
    key="advanced_process",
    actiontype=codeexec.execute(
        code="advanced_data_processing({{input_data}})"
    ),
    depends_on=[analyze_data_step],
    execution_mode=ExecutionMode(
        mode="all",
        if_condition=And(
            IsSuccess(step="analyze_data"),
            Or(
                GreaterThan(number="{{steps.analyze_data.data_size}}", value="1000"),
                TextContains(text="{{steps.analyze_data.complexity}}", value="high")
            )
        )
    )
)
```

### Dynamic Branching

Create workflows that branch based on runtime conditions:

```python
# Content processing workflow with dynamic routing
content_processor = Agent(
    name="content_router",
    description="Routes content based on type and characteristics"
)

# Classify content
classify_step = Step(
    agent=content_processor,
    key="classify",
    actiontype=llm.message(
        model="claude-sonnet-4",
        query="Classify this content: {{content}}",
        response_format={
            "Type": "json_schema",
            "Schema": {
                "schema": {
                    "type": "object",
                    "properties": {
                        "content_type": {"type": "string"},
                        "language": {"type": "string"},
                        "sentiment": {"type": "string"},
                        "priority": {"type": "string"}
                    }
                }
            }
        }
    )
)

# Route to different processing based on classification
process_text = Step(
    agent=content_processor,
    key="process_text",
    actiontype=llm.message(
        model="claude-sonnet-4",
        query="Process text content: {{content}}"
    ),
    depends_on=[classify_step],
    execution_mode=ExecutionMode(
        mode="all",
        if_condition=TextContains(
            text="{{steps.classify.content_type}}",
            value="text"
        )
    )
)

process_image = Step(
    agent=content_processor,
    key="process_image",
    actiontype=codeexec.execute(
        code="process_image_content({{content}})"
    ),
    depends_on=[classify_step],
    execution_mode=ExecutionMode(
        mode="all",
        if_condition=TextContains(
            text="{{steps.classify.content_type}}",
            value="image"
        )
    )
)

process_document = Step(
    agent=content_processor,
    key="process_document",
    actiontype=llm.message(
        model="claude-sonnet-4",
        query="Extract and summarize document: {{content}}"
    ),
    depends_on=[classify_step],
    execution_mode=ExecutionMode(
        mode="all",
        if_condition=TextContains(
            text="{{steps.classify.content_type}}",
            value="document"
        )
    )
)
```

## Iterative Processing

### Loop Over Collections

Process arrays or collections of data:

```python
# Batch email processing
email_processor = Agent(
    name="email_batch_processor",
    description="Processes multiple emails with iteration"
)

# Process each email in the batch
process_email_step = Step(
    agent=email_processor,
    key="process_email",
    actiontype=llm.message(
        model="claude-sonnet-4",
        query="Process this email: {{email}}",
        response_format={
            "Type": "json_schema",
            "Schema": {
                "schema": {
                    "type": "object",
                    "properties": {
                        "category": {"type": "string"},
                        "priority": {"type": "string"},
                        "action_required": {"type": "boolean"},
                        "summary": {"type": "string"}
                    }
                }
            }
        }
    ),
    execution_mode=ExecutionMode(
        mode="iterate_over",
        data="emails",  # Iterates over the emails array
        if_condition=IsSuccess()
    )
)

# Generate summary after processing all emails
summary_step = Step(
    agent=email_processor,
    key="summary",
    actiontype=llm.message(
        model="claude-sonnet-4",
        query="Generate summary of processed emails: {{steps.process_email.all_results}}"
    ),
    depends_on=[process_email_step]
)
```

### Conditional Iteration

Iterate with filtering conditions:

```python
# Process only high-priority items
priority_processor = Agent(
    name="priority_processor",
    description="Processes only high-priority items from a collection"
)

process_priority_item = Step(
    agent=priority_processor,
    key="process_item",
    actiontype=codeexec.execute(
        code="process_priority_item({{item}})"
    ),
    execution_mode=ExecutionMode(
        mode="iterate_over",
        data="items",
        if_condition=And(
            IsSuccess(),
            TextEquals(text="{{item.priority}}", value="high")
        )
    )
)
```

## Error Handling and Recovery

### Comprehensive Error Handling

Build robust workflows with multiple error recovery strategies:

```python
from erdo import ResultHandler

# Resilient data processing workflow
resilient_processor = Agent(
    name="resilient_processor",
    description="Data processor with comprehensive error handling"
)

# Main processing step
main_processing = Step(
    agent=resilient_processor,
    key="main_process",
    actiontype=codeexec.execute(
        code="{{generated_code}}",
        parameters={"data": "{{input_data}}"}
    )
)

# Success handler - normal flow continuation
success_handler = ResultHandler(
    step=main_processing,
    type="final",
    if_conditions=IsSuccess(),
    result_handler_order=0
)

# Retry handler for transient errors
retry_handler = ResultHandler(
    step=main_processing,
    type="intermediate",
    if_conditions=And(
        IsError(),
        LessThan(number="{{retry_count}}", value="3"),
        TextContains(text="{{error}}", value="timeout")
    ),
    result_handler_order=1
)

# Fallback handler for code generation errors
fallback_handler = ResultHandler(
    step=main_processing,
    type="intermediate",
    if_conditions=And(
        IsError(),
        TextContains(text="{{error}}", value="syntax")
    ),
    result_handler_order=2
)

# Critical error handler
critical_handler = ResultHandler(
    step=main_processing,
    type="final",
    if_conditions=And(
        IsError(),
        GreaterThan(number="{{retry_count}}", value="2")
    ),
    result_handler_order=3
)

# Retry logic with exponential backoff
retry_step = Step(
    result_handler=retry_handler,
    key="retry",
    actiontype=utils.echo(
        data={
            "retry_count": "{{increment retry_count}}",
            "backoff_time": "{{multiply retry_count 5}}",
            "last_error": "{{error}}"
        }
    )
)

# Regenerate code on syntax errors
regenerate_step = Step(
    result_handler=fallback_handler,
    key="regenerate",
    actiontype=llm.message(
        model="claude-sonnet-4",
        query="Fix the code error: {{error}}\nOriginal code: {{generated_code}}",
        system_prompt="Fix syntax and logical errors in the code."
    )
)

# Alert on critical failures
alert_step = Step(
    result_handler=critical_handler,
    key="alert",
    actiontype=utils.send_status(
        status="error",
        message="Critical processing failure after {{retry_count}} attempts",
        details={"final_error": "{{error}}"}
    )
)
```

## Advanced Workflow Patterns

### Pipeline Orchestration

Coordinate multiple agents in complex pipelines:

```python
# Multi-agent data pipeline
pipeline_orchestrator = Agent(
    name="data_pipeline",
    description="Orchestrates multi-stage data processing pipeline"
)

# Stage 1: Data ingestion and validation
ingest_data = Step(
    agent=pipeline_orchestrator,
    key="ingest",
    actiontype=bot.invoke(
        bot_name="data ingestion bot",
        parameters={"source": "{{data_source}}"}
    )
)

# Stage 2: Data cleaning and preprocessing
clean_data = Step(
    agent=pipeline_orchestrator,
    key="clean",
    actiontype=bot.invoke(
        bot_name="data cleaner bot",
        parameters={"raw_data": "{{steps.ingest.processed_data}}"}
    ),
    depends_on=[ingest_data]
)

# Stage 3: Feature engineering
engineer_features = Step(
    agent=pipeline_orchestrator,
    key="features",
    actiontype=bot.invoke(
        bot_name="feature engineering bot",
        parameters={"clean_data": "{{steps.clean.cleaned_data}}"}
    ),
    depends_on=[clean_data]
)

# Stage 4: Model training
train_model = Step(
    agent=pipeline_orchestrator,
    key="train",
    actiontype=bot.invoke(
        bot_name="model training bot",
        parameters={"features": "{{steps.features.feature_data}}"}
    ),
    depends_on=[engineer_features]
)

# Stage 5: Model validation
validate_model = Step(
    agent=pipeline_orchestrator,
    key="validate",
    actiontype=bot.invoke(
        bot_name="model validation bot",
        parameters={"model": "{{steps.train.trained_model}}"}
    ),
    depends_on=[train_model]
)
```

### Dynamic Workflow Generation

Create workflows that adapt based on runtime conditions:

```python
# Adaptive workflow generator
adaptive_agent = Agent(
    name="adaptive_workflow",
    description="Generates workflow steps based on data characteristics"
)

# Analyze requirements and generate workflow plan
plan_workflow = Step(
    agent=adaptive_agent,
    key="plan",
    actiontype=llm.message(
        model="claude-sonnet-4",
        query="Create processing plan for: {{requirements}}",
        response_format={
            "Type": "json_schema",
            "Schema": {
                "schema": {
                    "type": "object",
                    "properties": {
                        "steps": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "name": {"type": "string"},
                                    "action": {"type": "string"},
                                    "condition": {"type": "string"}
                                }
                            }
                        }
                    }
                }
            }
        }
    )
)

# Execute dynamic steps based on plan
execute_dynamic_step = Step(
    agent=adaptive_agent,
    key="execute_dynamic",
    actiontype=codeexec.execute(
        code="""
# Execute workflow steps dynamically
plan = {{steps.plan.steps}}
for step in plan:
    if evaluate_condition(step['condition']):
        execute_step(step['name'], step['action'])
"""
    ),
    depends_on=[plan_workflow],
    execution_mode=ExecutionMode(
        mode="iterate_over",
        data="steps.plan.steps"
    )
)
```

## Performance Optimization

### Optimized Workflow Patterns

<Tabs>
  <Tab title="Caching Strategy">
    ```python
    # Implement intelligent caching
    cached_processor = Step(
        agent=agent,
        key="cached_process",
        actiontype=codeexec.execute(
            code="""
    # Check cache first
    cache_key = generate_cache_key({{input_params}})
    cached_result = get_from_cache(cache_key)

    if cached_result:
        return cached_result
    else:
        result = expensive_processing({{input_data}})
        cache_result(cache_key, result)
        return result
    """
        )
    )
    ```

  </Tab>
  <Tab title="Batch Processing">
    ```python
    # Process items in optimized batches
    batch_processor = Step(
        agent=agent,
        key="batch_process",
        actiontype=codeexec.execute(
            code="""
    # Process in optimal batch sizes
    batch_size = calculate_optimal_batch_size({{data_size}})
    results = []

    for batch in chunk_data({{input_data}}, batch_size):
        batch_result = process_batch(batch)
        results.extend(batch_result)

    return aggregate_results(results)
    """
        )
    )
    ```

  </Tab>
  <Tab title="Resource Management">
    ```python
    # Manage computational resources
    resource_aware_step = Step(
        agent=agent,
        key="resource_aware",
        actiontype=codeexec.execute(
            code="""
    # Adjust processing based on available resources
    available_memory = get_available_memory()
    cpu_cores = get_cpu_count()

    if available_memory > 8 * GB:
        use_memory_intensive_algorithm()
    else:
        use_disk_based_algorithm()

    set_parallel_workers(min(cpu_cores, data_chunks))
    """
        )
    )
    ```

  </Tab>
</Tabs>

## Monitoring and Observability

### Workflow Monitoring

Add comprehensive monitoring to your workflows:

```python
# Monitored workflow with detailed tracking
monitored_agent = Agent(
    name="monitored_workflow",
    description="Workflow with comprehensive monitoring and logging"
)

# Add monitoring step
monitor_start = Step(
    agent=monitored_agent,
    key="monitor_start",
    actiontype=utils.send_status(
        status="started",
        message="Workflow started",
        details={
            "timestamp": "{{current_timestamp}}",
            "input_size": "{{data_size}}",
            "expected_duration": "{{estimated_time}}"
        }
    )
)

# Main processing with progress updates
main_process = Step(
    agent=monitored_agent,
    key="main_process",
    actiontype=codeexec.execute(
        code="""
# Processing with progress tracking
total_items = len({{input_data}})
processed = 0

for item in {{input_data}}:
    process_item(item)
    processed += 1

    # Update progress every 10%
    if processed % (total_items // 10) == 0:
        progress = (processed / total_items) * 100
        send_progress_update(f"Progress: {progress:.1f}%")

return {"processed_count": processed, "success_rate": calculate_success_rate()}
"""
    ),
    depends_on=[monitor_start]
)

# Monitor completion
monitor_end = Step(
    agent=monitored_agent,
    key="monitor_end",
    actiontype=utils.send_status(
        status="completed",
        message="Workflow completed successfully",
        details={
            "duration": "{{execution_time}}",
            "processed_items": "{{steps.main_process.processed_count}}",
            "success_rate": "{{steps.main_process.success_rate}}"
        }
    ),
    depends_on=[main_process]
)
```

<CardGroup cols={2}>
  <Card
    title="Error Handling"
    icon="exclamation-triangle"
    href="/sdk/error-handling"
  >
    Master error handling patterns and recovery strategies
  </Card>
  <Card title="Performance Tips" icon="tachometer-alt" href="/sdk/performance">
    Optimize your workflows for speed and efficiency
  </Card>
  <Card title="Testing Workflows" icon="vial" href="/advanced/testing">
    Learn how to test complex multi-step workflows
  </Card>
  <Card title="Real Examples" icon="code" href="/examples">
    See complete workflow implementations in action
  </Card>
</CardGroup>

## Best Practices

<AccordionGroup>
  <Accordion title="Workflow Design">
    - Keep individual steps focused and atomic - Use clear, descriptive step
    names and keys - Plan for failure scenarios from the start - Document
    complex conditional logic - Consider resource usage and optimization
  </Accordion>
  <Accordion title="Error Handling">
    - Implement multiple levels of error recovery - Use appropriate retry
    strategies (exponential backoff) - Distinguish between recoverable and fatal
    errors - Add comprehensive logging and monitoring - Test error scenarios
    thoroughly
  </Accordion>
  <Accordion title="Performance">
    - Use parallel processing where appropriate - Implement caching for
    expensive operations - Optimize batch sizes for your data - Monitor resource
    usage and bottlenecks - Consider workflow complexity vs. maintainability
  </Accordion>
</AccordionGroup>
