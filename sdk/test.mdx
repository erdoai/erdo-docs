---
title: "Test Module" 
description: "Test and validate agents locally and on the platform"
---

# Test Module

The Test module provides comprehensive testing capabilities for Erdo agents, supporting both local validation and live testing via the platform.

## Installation & Setup

```python
from erdo.test import AgentTest, LiveTest
from erdo.config import Config

# Verify configuration for live tests
config = Config()
if not config.is_authenticated():
    print("Please run 'erdo login' for live testing")
```

## Local Testing

### AgentTest Class

Use `AgentTest` for local validation and simulation without platform calls:

```python
from erdo import Agent
from erdo.actions import llm, memory
from erdo.test import AgentTest

# Create your agent
agent = Agent(
    name="test_agent", 
    description="Agent for testing"
)

step = agent.step(llm.message(
    model="claude-sonnet-4",
    context="Process: {{input}}"
))

# Create test instance
test = AgentTest(agent)

# Run validation
validation_results = test.validate()
print(f"Validation passed: {validation_results.passed}")

if not validation_results.passed:
    for error in validation_results.errors:
        print(f"Error: {error}")
```

### Local Simulation

```python
from erdo.test import AgentTest

# Simulate agent execution with test data
test = AgentTest(agent)

simulation_results = test.simulate(
    data={"input": "test data for simulation"}
)

print(f"Simulation status: {simulation_results.status}")
print(f"Steps executed: {len(simulation_results.step_results)}")

for step_result in simulation_results.step_results:
    print(f"Step: {step_result.step_name}")
    print(f"Status: {step_result.status}")
    print(f"Output: {step_result.output}")
```

## Live Testing

### LiveTest Class

Use `LiveTest` for running tests via the Erdo platform:

```python
from erdo.test import LiveTest

# Initialize live test client
live_test = LiveTest()

# Run all available tests
results = live_test.run_all()
print(f"Executed {len(results)} tests")

for result in results:
    print(f"Test: {result.test_name} - {result.status}")
```

### Test Specific Agent

```python
from erdo.test import LiveTest

live_test = LiveTest()

# Test specific agent
results = live_test.run_by_agent("data-analyzer")

for result in results:
    if result.status == "passed":
        print(f"‚úÖ {result.test_name}: {result.message}")
    else:
        print(f"‚ùå {result.test_name}: {result.error}")
```

## API Reference

### AgentTest

```python
class AgentTest:
    def __init__(self, agent: Agent):
        """Initialize local testing for an agent."""
    
    def validate(self) -> ValidationResult:
        """Validate agent structure and configuration."""
    
    def simulate(self, data: Dict) -> SimulationResult:
        """Simulate agent execution with test data."""
```

### LiveTest

```python
class LiveTest:
    def __init__(self, config: Optional[Config] = None):
        """Initialize live testing client."""
    
    def run_all(self) -> List[TestResult]:
        """Run all available tests."""
    
    def run_by_agent(self, agent_name: str) -> List[TestResult]:
        """Run tests for a specific agent."""
    
    def run_by_tag(self, tag: str) -> List[TestResult]:
        """Run tests matching a specific tag."""
```

### Result Types

```python
class ValidationResult:
    passed: bool                # Whether validation passed
    errors: List[str]          # Validation errors
    warnings: List[str]        # Validation warnings

class SimulationResult:
    status: str                # "success" or "error" 
    step_results: List[StepResult]  # Results for each step
    execution_time: float      # Simulation duration
    error: Optional[str]       # Error message if failed

class TestResult:
    test_name: str            # Name of the test
    status: str               # "passed", "failed", "skipped"
    message: Optional[str]    # Success message
    error: Optional[str]      # Error details
    duration: float           # Test execution time
    metadata: Dict            # Additional test data
```

## Testing Patterns

### Comprehensive Agent Testing

```python
from erdo import Agent
from erdo.actions import llm, memory
from erdo.test import AgentTest, LiveTest
from erdo.sync import Sync

def test_agent_thoroughly(agent: Agent):
    """Complete testing workflow for an agent."""
    
    print(f"Testing agent: {agent.name}")
    
    # 1. Local validation
    local_test = AgentTest(agent)
    validation = local_test.validate()
    
    if not validation.passed:
        print("‚ùå Local validation failed:")
        for error in validation.errors:
            print(f"  - {error}")
        return False
    
    print("‚úÖ Local validation passed")
    
    # 2. Local simulation
    simulation = local_test.simulate(data={
        "test_input": "sample data for testing"
    })
    
    if simulation.status != "success":
        print(f"‚ùå Simulation failed: {simulation.error}")
        return False
    
    print(f"‚úÖ Simulation passed ({len(simulation.step_results)} steps)")
    
    # 3. Sync to platform
    sync_result = Sync(agent)
    if sync_result.status != "success":
        print(f"‚ùå Sync failed: {sync_result.error}")
        return False
    
    print(f"‚úÖ Synced to platform: {sync_result.agent_key}")
    
    # 4. Live testing
    live_test = LiveTest()
    live_results = live_test.run_by_agent(agent.name)
    
    passed_tests = [r for r in live_results if r.status == "passed"]
    failed_tests = [r for r in live_results if r.status == "failed"]
    
    if failed_tests:
        print(f"‚ùå {len(failed_tests)} live tests failed:")
        for result in failed_tests:
            print(f"  - {result.test_name}: {result.error}")
        return False
    
    print(f"‚úÖ All {len(passed_tests)} live tests passed")
    return True

# Usage
my_agent = Agent(name="comprehensive_test")
# ... define agent steps ...

success = test_agent_thoroughly(my_agent)
if success:
    print("üéâ Agent ready for production!")
```

### Test-Driven Development

```python
from erdo import Agent
from erdo.actions import llm
from erdo.test import AgentTest

def create_agent_with_tdd():
    """Create agent using test-driven development."""
    
    # 1. Define expected behavior
    test_cases = [
        {"input": "hello", "expected_contains": "greeting"},
        {"input": "help me", "expected_contains": "assistance"},
        {"input": "analyze data", "expected_contains": "analysis"}
    ]
    
    # 2. Create initial agent
    agent = Agent(
        name="tdd_agent",
        description="Agent created with TDD approach"
    )
    
    response_step = agent.step(llm.message(
        model="claude-sonnet-4",
        context="Respond appropriately to: {{user_input}}"
    ))
    
    # 3. Test iteratively
    test = AgentTest(agent)
    
    for i, test_case in enumerate(test_cases):
        print(f"Running test case {i+1}...")
        
        simulation = test.simulate(data={"user_input": test_case["input"]})
        
        if simulation.status == "success":
            # Check if output contains expected content
            output = simulation.step_results[0].output
            if test_case["expected_contains"] in str(output).lower():
                print(f"‚úÖ Test case {i+1} passed")
            else:
                print(f"‚ùå Test case {i+1} failed: missing '{test_case['expected_contains']}'")
        else:
            print(f"‚ùå Test case {i+1} failed: {simulation.error}")
    
    return agent

# Usage
tdd_agent = create_agent_with_tdd()
```

### Regression Testing

```python
from erdo.test import LiveTest
from erdo.sync import Sync
import json
import time

class RegressionTester:
    """Automated regression testing for agents."""
    
    def __init__(self):
        self.live_test = LiveTest()
        self.baseline_results = {}
    
    def establish_baseline(self, agent_names: List[str]):
        """Establish performance baseline for agents."""
        print("Establishing performance baseline...")
        
        for agent_name in agent_names:
            results = self.live_test.run_by_agent(agent_name)
            self.baseline_results[agent_name] = {
                "test_count": len(results),
                "passed": len([r for r in results if r.status == "passed"]),
                "avg_duration": sum(r.duration for r in results) / len(results) if results else 0
            }
        
        # Save baseline
        with open("test_baseline.json", "w") as f:
            json.dump(self.baseline_results, f, indent=2)
        
        print(f"Baseline established for {len(agent_names)} agents")
    
    def run_regression_tests(self, agent_names: List[str]):
        """Run regression tests against baseline."""
        print("Running regression tests...")
        
        # Load baseline
        try:
            with open("test_baseline.json", "r") as f:
                baseline = json.load(f)
        except FileNotFoundError:
            print("No baseline found. Run establish_baseline() first.")
            return
        
        regression_issues = []
        
        for agent_name in agent_names:
            print(f"Testing {agent_name}...")
            
            current_results = self.live_test.run_by_agent(agent_name)
            baseline_data = baseline.get(agent_name, {})
            
            current_stats = {
                "test_count": len(current_results),
                "passed": len([r for r in current_results if r.status == "passed"]),
                "avg_duration": sum(r.duration for r in current_results) / len(current_results) if current_results else 0
            }
            
            # Check for regressions
            if current_stats["passed"] < baseline_data.get("passed", 0):
                regression_issues.append(f"{agent_name}: Test failures increased")
            
            if current_stats["avg_duration"] > baseline_data.get("avg_duration", 0) * 1.5:
                regression_issues.append(f"{agent_name}: Performance degraded significantly")
        
        if regression_issues:
            print("‚ùå Regression issues detected:")
            for issue in regression_issues:
                print(f"  - {issue}")
            return False
        else:
            print("‚úÖ No regressions detected")
            return True

# Usage
tester = RegressionTester()

# Establish baseline
agent_names = ["data-analyzer", "report-generator", "file-processor"]
tester.establish_baseline(agent_names)

# Later, after changes
regression_passed = tester.run_regression_tests(agent_names)
```

## Integration Examples

### CI/CD Testing

```python
#!/usr/bin/env python3
"""Agent testing for CI/CD pipeline."""

import sys
import json
from erdo.sync import Sync
from erdo.test import LiveTest

def ci_test_pipeline():
    """Complete CI/CD testing pipeline."""
    
    print("Starting CI/CD test pipeline...")
    
    # 1. Sync all agents
    print("Syncing agents...")
    sync_results = Sync.from_directory("agents/")
    
    failed_syncs = [r for r in sync_results if r.status != "success"]
    if failed_syncs:
        print(f"‚ùå {len(failed_syncs)} agents failed to sync:")
        for result in failed_syncs:
            print(f"  - {result.agent_name}: {result.error}")
        return False
    
    print(f"‚úÖ Synced {len(sync_results)} agents")
    
    # 2. Run live tests
    print("Running live tests...")
    live_test = LiveTest()
    all_results = live_test.run_all()
    
    failed_tests = [r for r in all_results if r.status == "failed"]
    if failed_tests:
        print(f"‚ùå {len(failed_tests)} tests failed:")
        for result in failed_tests:
            print(f"  - {result.test_name}: {result.error}")
        return False
    
    print(f"‚úÖ All {len(all_results)} tests passed")
    
    # 3. Generate test report
    report = {
        "synced_agents": len(sync_results),
        "test_results": len(all_results),
        "passed_tests": len([r for r in all_results if r.status == "passed"]),
        "total_duration": sum(r.duration for r in all_results)
    }
    
    with open("test_report.json", "w") as f:
        json.dump(report, f, indent=2)
    
    print("‚úÖ CI/CD pipeline completed successfully")
    return True

if __name__ == "__main__":
    success = ci_test_pipeline()
    sys.exit(0 if success else 1)
```

### Interactive Testing

```python
from erdo.test import AgentTest, LiveTest
from erdo.sync import Sync
import json

def interactive_testing_session():
    """Interactive testing session with user input."""
    
    print("üß™ Interactive Agent Testing Session")
    print("=" * 40)
    
    # Get agent from user
    agent_file = input("Enter agent file path: ").strip()
    if not agent_file:
        agent_file = "my_agent.py"
    
    # Sync agent
    print(f"\nSyncing agents from {agent_file}...")
    sync_results = Sync.from_file(agent_file)
    
    if not sync_results:
        print("‚ùå No agents found in file")
        return
    
    # Let user choose agent
    if len(sync_results) > 1:
        print(f"\nFound {len(sync_results)} agents:")
        for i, result in enumerate(sync_results):
            print(f"  {i+1}. {result.agent_name}")
        
        choice = int(input("Select agent (number): ")) - 1
        selected_agent = sync_results[choice]
    else:
        selected_agent = sync_results[0]
    
    print(f"\nSelected agent: {selected_agent.agent_name}")
    
    # Test menu
    while True:
        print("\nTesting options:")
        print("1. Run live tests")
        print("2. Custom test with input")
        print("3. View test history")
        print("4. Exit")
        
        choice = input("Choose option (1-4): ").strip()
        
        if choice == "1":
            print("\nRunning live tests...")
            live_test = LiveTest()
            results = live_test.run_by_agent(selected_agent.agent_name)
            
            for result in results:
                status_icon = "‚úÖ" if result.status == "passed" else "‚ùå"
                print(f"{status_icon} {result.test_name} ({result.duration:.2f}s)")
        
        elif choice == "2":
            test_input = input("Enter test input (JSON): ").strip()
            try:
                input_data = json.loads(test_input)
                # Here you would invoke the agent with the test data
                print(f"Testing with input: {input_data}")
                # result = Invoke.by_key(selected_agent.agent_key, parameters=input_data)
            except json.JSONDecodeError:
                print("‚ùå Invalid JSON input")
        
        elif choice == "3":
            print("Test history feature coming soon...")
        
        elif choice == "4":
            break
        
        else:
            print("Invalid choice")

# Run interactive session
interactive_testing_session()
```

## Best Practices

1. **Comprehensive Testing**: Use both local validation and live testing
2. **Test Early**: Run tests during development, not just before deployment
3. **Automated Testing**: Include testing in CI/CD pipelines
4. **Regression Testing**: Establish baselines and detect performance regressions
5. **Test Data Management**: Use realistic test data that covers edge cases
6. **Error Testing**: Test failure scenarios and error handling
7. **Documentation**: Document test cases and expected behaviors

## Troubleshooting

### Common Issues

**Local Validation Failures**
```python
from erdo.test import AgentTest

test = AgentTest(agent)
validation = test.validate()

if not validation.passed:
    print("Validation errors:")
    for error in validation.errors:
        print(f"  - {error}")
    
    print("Warnings:")
    for warning in validation.warnings:
        print(f"  - {warning}")
```

**Live Test Authentication**
```python
from erdo.config import Config

config = Config()
if not config.is_authenticated():
    print("Run: erdo login")
    print("Or set: ERDO_AUTH_TOKEN environment variable")
```

**Test Timeouts**
```python
from erdo.test import LiveTest

# Increase timeout for slow tests
live_test = LiveTest()
# Note: Timeout configuration will be added in future versions
```

**Missing Test Data**
```python
# Ensure test data is properly formatted
test_data = {
    "required_field": "value",
    "optional_field": "optional_value"
}

simulation = test.simulate(data=test_data)
```