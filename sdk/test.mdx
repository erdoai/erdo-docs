---
title: "Agent Testing"
description: "Write fast, parallel agent tests using the invoke() pattern"
---

# Agent Testing

Erdo provides a simple, fast testing pattern for agents using the `invoke()` function with parallel execution.

## Quick Start

### 1. Write Test Functions

Create a Python file with `agent_test_*` functions:

```python
from erdo import invoke
from erdo.test import text_contains

def agent_test_basic_query():
    """Test basic agent invocation."""
    response = invoke(
        "data-question-answerer",
        messages=[{"role": "user", "content": "What were Q4 sales?"}],
        datasets=["sales-q4-2024"],
        mode="replay",  # Free after first run!
    )

    # Assert invocation succeeded
    assert response.success, f"Invocation failed: {response.error}"

    # Assert on the result
    result_text = str(response.result)
    assert text_contains(result_text, "sales", case_sensitive=False)
```

### 2. Run Tests

```bash
# Run all tests in parallel
erdo agent-test tests/test_my_agent.py

# Verbose output
erdo agent-test tests/test_my_agent.py --verbose

# Limit parallel jobs
erdo agent-test tests/test_my_agent.py -j 4
```

### 3. See Results

```
Discovering tests in tests/test_my_agent.py...
Found 15 tests

Running tests in parallel...

======================================================================
AGENT TEST RESULTS
======================================================================

‚úÖ agent_test_csv_sales_total (0.45s)
‚úÖ agent_test_csv_product_breakdown (0.52s)
‚úÖ agent_test_postgres_customer_count (0.38s)
‚ùå agent_test_invalid_dataset (0.21s)

----------------------------------------------------------------------
Total: 15 | Passed: 14 | Failed: 1 | Duration: 2.3s
----------------------------------------------------------------------
```

## Why This Pattern?

**‚ö° 10x Faster**: Tests run in parallel automatically

**üÜì Free Testing**: Use `mode="replay"` for cached responses (free after first run)

**üéØ Simple**: Just name functions `agent_test_*` and use `invoke()`

**üìä Clean Output**: Nice summary with pass/fail counts and timing

**üîß No pytest**: Pure Python, easy to debug

## Test Modes

Control how bot actions are executed in tests:

| Mode | Description | Cost | Use Case |
|------|-------------|------|----------|
| **live** | Real API calls | $$$ per run | Integration tests, fresh data |
| **replay** | Cached responses | $$$ first run, FREE after | Unit tests, CI/CD |
| **manual** | Developer mocks | FREE always | Deterministic tests, offline |

### Live Mode (Default)
Runs against real backend with LLM API (costs $):

```python
response = invoke("my-agent", messages=[...])  # mode="live" is default
```

**Use for:** Integration tests requiring fresh data

### Replay Mode (Recommended)
Uses cached responses - **first run costs $, subsequent runs FREE**:

```python
def agent_test_basic():
    response = invoke("my-agent", messages=[...], mode="replay")
    assert response.success
```

**Cache behavior:**
- First run: Executes live and caches response
- Subsequent runs: Returns cached response (FREE!)
- Auto-invalidates when bot is updated

**Use for:** Most agent tests, CI/CD pipelines

#### Replay Mode with Refresh

Force cache refresh while staying in replay mode:

```python
def agent_test_with_fresh_data():
    # Bypass cache and get fresh response
    response = invoke(
        "my-agent",
        messages=[...],
        mode={"mode": "replay", "refresh": True}
    )
    assert response.success
```

**Use for:**
- Testing with fresh data without leaving replay mode
- Updating cached responses after bot changes
- Testing cache refresh behavior

### Manual Mode
Developer-provided mock responses - always free:

```python
def agent_test_with_mocks():
    response = invoke(
        "my-agent",
        messages=[...],
        mode="manual",
        manual_mocks={
            "llm.message": {
                "status": "success",
                "output": {"content": "Mocked response"}
            }
        }
    )
    assert response.success
    assert "Mocked" in str(response.result)
```

**Use for:**
- Testing error handling
- Deterministic test outcomes
- Offline development
- Fast unit tests without API calls

## Test Helpers

Import assertion helpers from `erdo.test`:

```python
from erdo.test import (
    text_contains,      # Check if text contains substring
    text_equals,        # Check exact match
    text_matches,       # Check regex pattern
    json_path_equals,   # Check JSON path value
    json_path_exists,   # Check if JSON path exists
    has_dataset,        # Check if dataset is present
)
```

### text_contains

Check if text contains a substring:

```python
result_text = str(response.result)

assert text_contains(result_text, "expected")
assert text_contains(result_text, "EXPECTED", case_sensitive=False)
```

### text_equals

Check exact text match:

```python
assert text_equals(result_text, "exact match")
assert text_equals(result_text, "EXACT MATCH", case_sensitive=False)
```

### text_matches

Check regex pattern:

```python
import re

assert text_matches(result_text, r"\d+ customers")
assert text_matches(result_text, r"^hello", re.IGNORECASE)
```

### json_path_equals

Check JSON path values:

```python
data = {"user": {"name": "Alice", "age": 30}}

assert json_path_equals(data, "user.name", "Alice")
assert json_path_equals(data, "user.age", 30)
```

### json_path_exists

Check if JSON path exists:

```python
assert json_path_exists(data, "user.name")
assert not json_path_exists(data, "user.email")
```

### has_dataset

Check if dataset is present in response:

```python
assert has_dataset(response)
assert has_dataset(response, dataset_id="abc-123")
assert has_dataset(response, dataset_key="sales_data")
```

## Complete Example

```python
"""
Agent tests for data question answerer.

To run:
  erdo agent-test tests/test_data_question_answerer.py
"""

from erdo import invoke
from erdo.test import text_contains, text_equals

# Agent key constant
AGENT_KEY = "data-question-answerer"

def agent_test_csv_sales_total():
    """Test CSV sales total aggregation."""
    response = invoke(
        AGENT_KEY,
        messages=[{"role": "user", "content": "What were total sales in October?"}],
        datasets=["sales-q4-2024"],
        mode="replay",
    )

    assert response.success
    result_text = str(response.result)
    assert text_contains(result_text, "October", case_sensitive=False)
    assert text_contains(result_text, "total", case_sensitive=False)


def agent_test_csv_product_breakdown():
    """Test CSV product category breakdown."""
    response = invoke(
        AGENT_KEY,
        messages=[{"role": "user", "content": "Show sales by product category"}],
        datasets=["sales-q4-2024"],
        mode="replay",
    )

    assert response.success
    result_text = str(response.result)
    assert text_contains(result_text, "category", case_sensitive=False)


def agent_test_with_parameters():
    """Test passing custom parameters."""
    response = invoke(
        AGENT_KEY,
        messages=[{"role": "user", "content": "Analyze the sales data"}],
        datasets=["sales-q4-2024"],
        parameters={
            "analysis_type": "trend",
            "time_period": "weekly"
        },
        mode="replay",
    )

    assert response.success
    result_text = str(response.result)
    assert text_contains(result_text, "trend", case_sensitive=False) or \
           text_contains(result_text, "weekly", case_sensitive=False)


def agent_test_multi_dataset():
    """Test with multiple datasets."""
    response = invoke(
        AGENT_KEY,
        messages=[{
            "role": "user",
            "content": "Is there a correlation between traffic and sales?"
        }],
        datasets=["sales-q4-2024", "ga-main-property"],
        mode="replay",
    )

    assert response.success
    result_text = str(response.result)
    assert text_contains(result_text, "correlation", case_sensitive=False)


def agent_test_empty_result():
    """Test handling of empty results."""
    response = invoke(
        AGENT_KEY,
        messages=[{
            "role": "user",
            "content": "Show products costing over $1,000,000"
        }],
        datasets=["sales-q4-2024"],
        mode="replay",
    )

    assert response.success
    result_text = str(response.result)
    # Should gracefully indicate no results
    assert text_contains(result_text, "no", case_sensitive=False)
```

## Best Practices

### 1. Use Descriptive Names

```python
# Good - clear purpose
def agent_test_csv_sales_aggregation():
    ...

# Bad - unclear
def agent_test_test1():
    ...
```

### 2. Choose the Right Mode

```python
# Good - replay mode for most tests (fast, free after first run)
response = invoke("my-agent", messages=[...], mode="replay")

# Good - manual mode for deterministic tests
response = invoke("my-agent", messages=[...], mode="manual",
                  manual_mocks={"llm.message": {...}})

# Good - refresh when you need fresh data
response = invoke("my-agent", messages=[...], mode={"mode": "replay", "refresh": True})

# Avoid - live mode costs $ every test run
response = invoke("my-agent", messages=[...])  # mode="live" is expensive
```

**Mode selection guide:**
- **Replay mode**: Default choice for most tests (free after first run)
- **Manual mode**: When you need deterministic, controlled responses
- **Refresh**: When you need to update cached responses
- **Live mode**: Only for integration tests requiring fresh data

### 3. Add Good Error Messages

```python
# Good
assert response.success, f"Invocation failed: {response.error}"
assert text_contains(result, "sales"), "Response should mention sales"

# Bad
assert response.success
assert "sales" in result
```

### 4. Test Edge Cases

```python
def agent_test_empty_dataset():
    """Test with empty dataset."""
    response = invoke("my-agent", datasets=[], mode="replay")
    assert response.success

def agent_test_long_message():
    """Test with long message."""
    long_text = "Lorem ipsum " * 100
    response = invoke("my-agent", messages=[...], mode="replay")
    assert response.success

def agent_test_special_characters():
    """Test with special characters."""
    response = invoke(
        "my-agent",
        messages=[{"role": "user", "content": "Test: $100 & 50% @ #tag"}],
        mode="replay"
    )
    assert response.success
```

### 5. Group Related Tests

```python
# CSV Tests
def agent_test_csv_basic():
    ...

def agent_test_csv_aggregation():
    ...

def agent_test_csv_filtering():
    ...

# Database Tests
def agent_test_postgres_query():
    ...

def agent_test_postgres_join():
    ...
```

## CLI Reference

```bash
# Run all tests in a file
erdo agent-test tests/test_my_agent.py

# Verbose output (show full error traces)
erdo agent-test tests/test_my_agent.py --verbose

# Limit parallel jobs
erdo agent-test tests/test_my_agent.py -j 4

# Refresh cached responses (force re-execution in replay mode)
erdo agent-test tests/test_my_agent.py --refresh

# Combine flags
erdo agent-test tests/test_my_agent.py --refresh --verbose -j 8

# Help
erdo agent-test --help
```

**CLI Flags:**

- `-v, --verbose`: Show detailed error traces
- `-j, --jobs <N>`: Number of parallel jobs (default: auto)
- `-r, --refresh`: Force refresh cached responses in replay mode

**Refresh Flag:**

The `--refresh` flag automatically forces all tests using `mode="replay"` to bypass the cache and get fresh responses. This is useful when:
- You've updated your bot logic and want to refresh cached responses
- You need fresh data from APIs without modifying test code
- You want to verify that tests still pass with current LLM behavior

**Important:** Tests must use `mode="replay"` for the refresh flag to work. Tests using `mode="live"` or `mode="manual"` are unaffected.

## Python API

You can also run tests programmatically:

```python
from erdo.test import run_tests

# Run tests from a file
exit_code = run_tests(
    "tests/test_my_agent.py",
    verbose=True,
    max_workers=4
)

# exit_code is 0 if all tests passed, 1 if any failed
```

## Performance

**Old Pattern (Sequential)**
```
15 tests √ó 2s each = 30 seconds total
```

**New Pattern (Parallel)**
```
15 tests in parallel = ~2-3 seconds total
```

**10x faster! üöÄ**

## Troubleshooting

### No tests found

Make sure your test functions start with `agent_test_`:

```python
# Correct
def agent_test_my_feature():
    ...

# Wrong - won't be discovered
def test_my_feature():
    ...

def my_test():
    ...
```

### Tests failing with "Bot not found"

Make sure the agent is synced to the backend:

```bash
erdo sync-agent path/to/agent.py
```

### Authentication errors

Login first:

```bash
erdo login
```

Or set environment variables:

```bash
export ERDO_ENDPOINT="https://api.erdo.ai"
export ERDO_AUTH_TOKEN="your-token"
```

### Slow test execution

Use replay mode for faster, free testing:

```python
# Slow - hits LLM API every time
response = invoke("my-agent", messages=[...])

# Fast - cached after first run
response = invoke("my-agent", messages=[...], mode="replay")
```

### Import errors

Make sure the SDK is installed:

```bash
cd erdo-agents
uv pip install -e ../erdo-python-sdk
```

## See Also

- [Invoke Module](/sdk/invoke) - Learn about the invoke() function
- [CLI Commands](/cli/commands) - Complete CLI reference
- [Examples](/examples/data-analysis) - More testing examples
