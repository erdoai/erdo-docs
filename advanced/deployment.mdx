---
title: "Production Deployment"
description: "Deploy and manage Erdo agents in production environments with best practices for scalability and reliability"
---

# Production Deployment

Learn how to deploy, monitor, and scale Erdo agents in production environments with enterprise-grade reliability and performance.

## Pre-Deployment Checklist

<Steps>
  <Step title="Code Review & Testing">
    Ensure your agents are thoroughly tested and code reviewed
  </Step>
  <Step title="Performance Optimization">
    Optimize for production workloads and resource constraints
  </Step>
  <Step title="Security Validation">
    Validate security configurations and access controls
  </Step>
  <Step title="Monitoring Setup">
    Configure monitoring, logging, and alerting systems
  </Step>
  <Step title="Deployment Strategy">
    Plan your deployment approach and rollback procedures
  </Step>
</Steps>

### Development to Production Workflow

```bash
# 1. Final testing in development
erdo test my-agent --input production-sample.csv --verbose

# 2. Validate agent configuration
erdo sync --validate-only

# 3. Preview deployment changes
erdo sync --dry-run

# 4. Deploy to staging environment
erdo sync --name-suffix "_staging"

# 5. Run integration tests
erdo test my-agent_staging --parallel 5

# 6. Deploy to production
erdo sync

# 7. Monitor deployment
erdo logs my-agent --follow --level info
```

## Production Configuration

### Agent Configuration for Production

```python
from erdo import Agent, Step
from erdo.actions import llm, codeexec

production_agent = Agent(
    name="production_data_processor",
    description="Production-ready data processing agent",
    visibility="organization",
    configuration={
        # Resource limits
        "max_execution_time": 1800,  # 30 minutes
        "memory_limit": "4GB",
        "cpu_limit": "4 cores",

        # Reliability settings
        "max_retries": 3,
        "retry_backoff": "exponential",
        "timeout_strategy": "graceful",

        # Security settings
        "network_access": "restricted",
        "allowed_domains": [
            "api.company.com",
            "data.warehouse.com"
        ],

        # Monitoring settings
        "enable_metrics": True,
        "log_level": "info",
        "trace_execution": True
    },
    metadata={
        "environment": "production",
        "version": "2.1.0",
        "owner": "data-team",
        "cost_center": "analytics",
        "sla_tier": "high"
    }
)
```

### Environment-Specific Configuration

<Tabs>
  <Tab title="Production">
    ```python
    # Production configuration
    production_config = {
        "environment": "production",
        "log_level": "info",
        "enable_debug": False,
        "resource_limits": {
            "memory": "8GB",
            "cpu": "8 cores",
            "timeout": 3600
        },
        "retry_policy": {
            "max_attempts": 5,
            "backoff_factor": 2,
            "jitter": True
        },
        "security": {
            "encrypt_outputs": True,
            "audit_logging": True,
            "access_control": "strict"
        }
    }
    ```
  </Tab>
  <Tab title="Staging">
    ```python
    # Staging configuration
    staging_config = {
        "environment": "staging",
        "log_level": "debug",
        "enable_debug": True,
        "resource_limits": {
            "memory": "4GB",
            "cpu": "4 cores",
            "timeout": 1800
        },
        "retry_policy": {
            "max_attempts": 3,
            "backoff_factor": 1.5
        },
        "security": {
            "encrypt_outputs": False,
            "audit_logging": True,
            "access_control": "permissive"
        }
    }
    ```
  </Tab>
  <Tab title="Development">
    ```python
    # Development configuration
    development_config = {
        "environment": "development",
        "log_level": "debug",
        "enable_debug": True,
        "resource_limits": {
            "memory": "2GB",
            "cpu": "2 cores",
            "timeout": 600
        },
        "retry_policy": {
            "max_attempts": 1,
            "backoff_factor": 1
        },
        "security": {
            "encrypt_outputs": False,
            "audit_logging": False,
            "access_control": "none"
        }
    }
    ```
  </Tab>
</Tabs>

## Scalability and Performance

### High-Volume Processing

Configure agents for high-throughput scenarios:

```python
high_volume_agent = Agent(
    name="high_volume_processor",
    description="Processes large volumes of data efficiently",
    configuration={
        # Batch processing settings
        "batch_size": 1000,
        "parallel_batches": 10,
        "queue_size": 10000,

        # Memory management
        "memory_limit": "16GB",
        "gc_frequency": "aggressive",
        "memory_threshold": 0.8,

        # Performance optimization
        "connection_pooling": True,
        "cache_size": "1GB",
        "prefetch_data": True,

        # Load balancing
        "max_concurrent_executions": 50,
        "load_balancing_strategy": "round_robin"
    }
)

# Batch processing step
batch_processor = Step(
    agent=high_volume_agent,
    key="process_batch",
    actiontype=codeexec.execute(
        code="""
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor

async def process_item_async(item):
    # Async processing logic
    result = await async_process(item)
    return result

async def process_batch(items):
    # Process items concurrently
    tasks = [process_item_async(item) for item in items]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # Handle errors and successful results
    successful = [r for r in results if not isinstance(r, Exception)]
    errors = [r for r in results if isinstance(r, Exception)]

    return {
        "processed": len(successful),
        "errors": len(errors),
        "results": successful
    }

# Main processing logic
batch_data = {{input_batch}}
result = asyncio.run(process_batch(batch_data))
"""
    ),
    execution_mode=ExecutionMode(
        mode="iterate_over",
        data="batches",
        parallel_limit=10
    )
)
```

### Auto-Scaling Configuration

```python
# Auto-scaling based on workload
autoscaling_config = {
    "scaling_policy": {
        "min_instances": 2,
        "max_instances": 20,
        "target_cpu_utilization": 70,
        "target_memory_utilization": 80,
        "scale_up_threshold": 5,      # minutes
        "scale_down_threshold": 15,   # minutes
        "cooldown_period": 300        # seconds
    },
    "load_balancing": {
        "algorithm": "least_connections",
        "health_check_interval": 30,
        "unhealthy_threshold": 3,
        "healthy_threshold": 2
    }
}
```

## Monitoring and Observability

### Comprehensive Monitoring Setup

```python
# Monitoring and metrics collection
monitoring_step = Step(
    agent=production_agent,
    key="monitor",
    actiontype=utils.send_status(
        status="monitoring",
        message="Collecting performance metrics",
        details={
            "execution_time": "{{execution_duration}}",
            "memory_usage": "{{memory_peak}}",
            "cpu_utilization": "{{cpu_average}}",
            "throughput": "{{items_per_second}}",
            "error_rate": "{{error_percentage}}",
            "queue_size": "{{pending_items}}"
        }
    )
)

# Custom metrics tracking
metrics_step = Step(
    agent=production_agent,
    key="metrics",
    actiontype=codeexec.execute(
        code="""
import time
import psutil
from datetime import datetime

# Collect system metrics
metrics = {
    "timestamp": datetime.utcnow().isoformat(),
    "system": {
        "cpu_percent": psutil.cpu_percent(interval=1),
        "memory_percent": psutil.virtual_memory().percent,
        "disk_usage": psutil.disk_usage('/').percent
    },
    "application": {
        "processing_rate": {{items_processed}} / {{execution_time}},
        "error_rate": {{errors}} / {{total_items}} * 100,
        "cache_hit_rate": {{cache_hits}} / {{cache_requests}} * 100,
        "response_time": {{average_response_time}}
    },
    "business": {
        "total_processed": {{items_processed}},
        "data_quality_score": {{quality_score}},
        "sla_compliance": {{sla_met_percentage}}
    }
}

# Send to monitoring system
send_metrics_to_datadog(metrics)
log_metrics_to_cloudwatch(metrics)

return metrics
"""
    )
)
```

### Alerting and Notifications

<CardGroup cols={2}>
  <Card title="Performance Alerts" icon="tachometer-alt">
    - CPU usage > 80% - Memory usage > 90% - Response time > 30s - Queue size >
    10,000
  </Card>
  <Card title="Business Alerts" icon="exclamation-triangle">
    - Error rate > 5% - SLA violations - Data quality issues - Processing delays
  </Card>
  <Card title="System Alerts" icon="server">
    - Service unavailable - Disk space low - Network connectivity issues -
    Authentication failures
  </Card>
  <Card title="Custom Alerts" icon="bell">
    - Business rule violations - Anomaly detection - Threshold breaches -
    Compliance issues
  </Card>
</CardGroup>

### Log Management

```python
# Structured logging configuration
logging_config = {
    "version": 1,
    "formatters": {
        "json": {
            "format": "%(asctime)s %(name)s %(levelname)s %(message)s",
            "class": "pythonjsonlogger.jsonlogger.JsonFormatter"
        }
    },
    "handlers": {
        "console": {
            "class": "logging.StreamHandler",
            "formatter": "json",
            "level": "INFO"
        },
        "file": {
            "class": "logging.handlers.RotatingFileHandler",
            "filename": "/var/log/erdo/agent.log",
            "maxBytes": 100000000,  # 100MB
            "backupCount": 5,
            "formatter": "json"
        },
        "syslog": {
            "class": "logging.handlers.SysLogHandler",
            "address": ("log-server", 514),
            "formatter": "json"
        }
    },
    "loggers": {
        "erdo": {
            "handlers": ["console", "file", "syslog"],
            "level": "INFO",
            "propagate": False
        }
    }
}
```

## Security and Compliance

### Security Best Practices

```python
# Security configuration
security_config = {
    "authentication": {
        "method": "oauth2",
        "token_expiry": 3600,
        "refresh_enabled": True,
        "mfa_required": True
    },
    "authorization": {
        "rbac_enabled": True,
        "permissions": [
            "agent.execute",
            "data.read",
            "logs.read"
        ],
        "resource_access": "restricted"
    },
    "data_protection": {
        "encryption_at_rest": True,
        "encryption_in_transit": True,
        "key_rotation": "monthly",
        "data_masking": True
    },
    "audit": {
        "log_all_access": True,
        "log_data_changes": True,
        "retention_period": "7 years",
        "compliance_framework": "SOX"
    }
}

# Secure agent implementation
secure_agent = Agent(
    name="secure_processor",
    description="Security-compliant data processor",
    configuration=security_config
)

# Data sanitization step
sanitize_step = Step(
    agent=secure_agent,
    key="sanitize",
    actiontype=codeexec.execute(
        code="""
import re
import hashlib

def sanitize_pii(data):
    # Remove or mask PII data
    sanitized = data.copy()

    # Email masking
    sanitized = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
                      '[EMAIL_MASKED]', sanitized)

    # Phone number masking
    sanitized = re.sub(r'\b\d{3}-\d{3}-\d{4}\b', '[PHONE_MASKED]', sanitized)

    # SSN masking
    sanitized = re.sub(r'\b\d{3}-\d{2}-\d{4}\b', '[SSN_MASKED]', sanitized)

    return sanitized

# Hash sensitive identifiers
def hash_identifier(identifier):
    return hashlib.sha256(identifier.encode()).hexdigest()[:16]

# Process data securely
cleaned_data = sanitize_pii({{input_data}})
return {"sanitized_data": cleaned_data}
"""
    )
)
```

### Compliance Frameworks

<Tabs>
  <Tab title="GDPR Compliance">
    ```python
    gdpr_config = {
        "data_minimization": True,
        "purpose_limitation": True,
        "consent_management": True,
        "right_to_erasure": True,
        "data_portability": True,
        "privacy_by_design": True,
        "retention_policies": {
            "personal_data": "2 years",
            "anonymized_data": "7 years"
        }
    }
    ```
  </Tab>
  <Tab title="HIPAA Compliance">
    ```python
    hipaa_config = {
        "minimum_necessary": True,
        "access_controls": "strict",
        "audit_logs": "comprehensive",
        "encryption": "aes256",
        "data_backup": "encrypted",
        "breach_notification": True,
        "business_associate": True
    }
    ```
  </Tab>
  <Tab title="SOX Compliance">
    ```python
    sox_config = {
        "financial_reporting": True,
        "internal_controls": True,
        "data_integrity": True,
        "audit_trail": "immutable",
        "segregation_of_duties": True,
        "change_management": True,
        "documentation": "complete"
    }
    ```
  </Tab>
</Tabs>

## Disaster Recovery and Business Continuity

### Backup and Recovery

```python
# Backup configuration
backup_config = {
    "schedule": {
        "full_backup": "weekly",
        "incremental_backup": "daily",
        "transaction_log_backup": "hourly"
    },
    "retention": {
        "daily_backups": 30,
        "weekly_backups": 12,
        "monthly_backups": 12,
        "yearly_backups": 7
    },
    "storage": {
        "primary": "s3://backup-primary",
        "secondary": "s3://backup-secondary",
        "cross_region": True,
        "encryption": "aes256"
    },
    "testing": {
        "recovery_test_frequency": "monthly",
        "rto_target": "4 hours",
        "rpo_target": "1 hour"
    }
}

# Automated backup step
backup_step = Step(
    agent=production_agent,
    key="backup",
    actiontype=codeexec.execute(
        code="""
import boto3
import datetime
from pathlib import Path

# Create backup
backup_data = {
    "agent_state": {{agent_state}},
    "execution_history": {{execution_history}},
    "configuration": {{agent_configuration}},
    "timestamp": datetime.utcnow().isoformat()
}

# Upload to S3
s3_client = boto3.client('s3')
backup_key = f"backups/{datetime.utcnow().strftime('%Y/%m/%d')}/agent_backup.json"

s3_client.put_object(
    Bucket='production-backups',
    Key=backup_key,
    Body=json.dumps(backup_data),
    ServerSideEncryption='AES256'
)

return {"backup_location": backup_key, "status": "success"}
"""
    )
)
```

### High Availability Setup

```python
# High availability configuration
ha_config = {
    "deployment": {
        "regions": ["us-east-1", "us-west-2", "eu-west-1"],
        "availability_zones": 3,
        "load_balancer": "application",
        "health_checks": True
    },
    "failover": {
        "automatic": True,
        "failover_time": "< 5 minutes",
        "health_check_interval": 30,
        "failure_threshold": 3
    },
    "data_replication": {
        "sync_replication": True,
        "cross_region": True,
        "consistency": "strong"
    }
}
```

## Cost Optimization

### Resource Management

```python
# Cost optimization strategies
cost_optimization = {
    "resource_scheduling": {
        "scale_down_nights": True,
        "scale_down_weekends": True,
        "business_hours": "9AM-6PM EST",
        "timezone": "America/New_York"
    },
    "instance_optimization": {
        "right_sizing": True,
        "spot_instances": "non_critical",
        "reserved_instances": "predictable_workloads",
        "auto_shutdown": "idle_timeout"
    },
    "data_lifecycle": {
        "hot_storage": "30 days",
        "warm_storage": "90 days",
        "cold_storage": "1 year",
        "archive_storage": "7 years"
    }
}

# Cost monitoring step
cost_monitoring = Step(
    agent=production_agent,
    key="cost_monitor",
    actiontype=codeexec.execute(
        code="""
import boto3
from datetime import datetime, timedelta

# Get cost data
ce_client = boto3.client('ce')
end_date = datetime.utcnow().date()
start_date = end_date - timedelta(days=30)

response = ce_client.get_cost_and_usage(
    TimePeriod={
        'Start': start_date.strftime('%Y-%m-%d'),
        'End': end_date.strftime('%Y-%m-%d')
    },
    Granularity='DAILY',
    Metrics=['BlendedCost'],
    GroupBy=[
        {'Type': 'DIMENSION', 'Key': 'SERVICE'},
        {'Type': 'TAG', 'Key': 'Environment'}
    ]
)

# Analyze costs
total_cost = sum(float(result['Total']['BlendedCost']['Amount'])
                for result in response['ResultsByTime'])

cost_by_service = {}
for result in response['ResultsByTime']:
    for group in result['Groups']:
        service = group['Keys'][0]
        cost = float(group['Metrics']['BlendedCost']['Amount'])
        cost_by_service[service] = cost_by_service.get(service, 0) + cost

return {
    "total_monthly_cost": total_cost,
    "cost_by_service": cost_by_service,
    "cost_per_execution": total_cost / {{total_executions}},
    "optimization_recommendations": generate_cost_recommendations(cost_by_service)
}
"""
    )
)
```

## Deployment Strategies

<AccordionGroup>
  <Accordion title="Blue-Green Deployment">
    Deploy new version alongside current version, then switch traffic:

    ```bash
    # Deploy new version
    erdo sync --name-suffix "_v2"

    # Test new version
    erdo test my-agent_v2 --production-data

    # Switch traffic (done at load balancer level)
    update-load-balancer --target my-agent_v2

    # Monitor and rollback if needed
    erdo logs my-agent_v2 --follow
    ```

  </Accordion>

  <Accordion title="Canary Deployment">
    Gradually roll out new version to subset of traffic:

    ```bash
    # Deploy canary version
    erdo sync --name-suffix "_canary"

    # Route 10% of traffic to canary
    update-traffic-split --canary 10%

    # Monitor metrics and gradually increase
    erdo logs my-agent_canary --level error
    ```

  </Accordion>

  <Accordion title="Rolling Deployment">
    Update instances one by one to minimize downtime:

    ```bash
    # Update instances in batches
    for batch in $(get-instance-batches); do
        update-batch $batch
        wait-for-health-check $batch
        sleep 60
    done
    ```

  </Accordion>
</AccordionGroup>

<CardGroup cols={2}>
  <Card title="Security Guide" icon="shield-alt" href="/advanced/security">
    Comprehensive security best practices and compliance
  </Card>
  <Card
    title="Performance Tuning"
    icon="tachometer-alt"
    href="/advanced/performance"
  >
    Optimize agents for production workloads
  </Card>
  <Card title="Monitoring Setup" icon="chart-line" href="/advanced/monitoring">
    Set up comprehensive monitoring and alerting
  </Card>
  <Card title="Troubleshooting" icon="tools" href="/advanced/troubleshooting">
    Common production issues and solutions
  </Card>
</CardGroup>
