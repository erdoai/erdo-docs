---
title: "Data Analysis Examples"
description: "Real-world examples of data analysis agents for business insights and automation"
---

# Data Analysis Examples

Comprehensive examples showing how to build powerful data analysis agents that can process, analyze, and generate insights from various data sources using the new agent-centric API.

## Sales Data Analyzer

Analyze sales performance and generate actionable business insights:

```python
from erdo import Agent
from erdo.actions import llm, codeexec, memory, utils
from erdo.conditions import IsSuccess, GreaterThan, IsError

sales_analyzer = Agent(
    name="sales_data_analyzer",
    description="Analyzes sales data and generates business insights"
)

# Step 1: Data preprocessing and validation
preprocess_step = sales_analyzer.step(
    codeexec.execute(
        code_files=[{
            "filename": "preprocess.py",
            "content": """
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

# Load and validate sales data
data = pd.read_csv('{{data_source}}')

# Data quality checks
missing_data = data.isnull().sum()
duplicate_rows = data.duplicated().sum()
date_range = (data['date'].min(), data['date'].max())

# Clean and prepare data
data['date'] = pd.to_datetime(data['date'])
data = data.dropna(subset=['amount', 'customer_id'])
data['month'] = data['date'].dt.to_period('M')

# Calculate key metrics
total_revenue = data['amount'].sum()
unique_customers = data['customer_id'].nunique()
avg_order_value = data['amount'].mean()

result = {
    "cleaned_data": data.to_dict('records'),
    "data_quality": {
        "missing_values": missing_data.to_dict(),
        "duplicates": int(duplicate_rows),
        "date_range": [str(date_range[0]), str(date_range[1])]
    },
    "summary_metrics": {
        "total_revenue": float(total_revenue),
        "unique_customers": int(unique_customers),
        "avg_order_value": float(avg_order_value),
        "total_transactions": len(data)
    }
}

import json
print(json.dumps(result))
"""
        }],
        parameters={"data_source": "{{data_source}}"}
    ),
    key="preprocess"
)

# Step 2: Advanced analytics and trend analysis
analysis_step = sales_analyzer.step(
    llm.message(
        model="claude-sonnet-4",
        system_prompt="You are an expert business analyst. Analyze the provided sales data and identify trends, patterns, and actionable insights.",
        query="Analyze this sales data: {{preprocess_step.output.summary_metrics}}",
        response_format={
            "Type": "json_schema",
            "Schema": {
                "type": "object",
                "required": ["insights", "recommendations", "key_findings"],
                "properties": {
                    "insights": {
                        "type": "array",
                        "items": {"type": "string"}
                    },
                    "recommendations": {
                        "type": "array",
                        "items": {"type": "string"}
                    },
                    "key_findings": {
                        "type": "object",
                        "properties": {
                            "revenue_trend": {"type": "string"},
                            "top_performing_segments": {"type": "array"},
                            "seasonal_patterns": {"type": "string"}
                        }
                    }
                }
            }
        }
    ),
    depends_on=preprocess_step,
    key="analyze"
)

# Step 3: Statistical analysis
stats_step = sales_analyzer.step(
    codeexec.execute(
        code_files=[{
            "filename": "statistical_analysis.py",
            "content": """
import pandas as pd
import numpy as np
from scipy import stats
from sklearn.linear_model import LinearRegression
import json

# Load cleaned data
data = {{preprocess_step.output.cleaned_data}}
df = pd.DataFrame(data)

# Time series analysis
df['date'] = pd.to_datetime(df['date'])
monthly_sales = df.groupby(df['date'].dt.to_period('M'))['amount'].agg(['sum', 'count', 'mean'])
growth_rate = monthly_sales['sum'].pct_change().mean()

# Customer segmentation
customer_metrics = df.groupby('customer_id')['amount'].agg(['sum', 'count', 'mean'])
customer_metrics['segment'] = pd.cut(
    customer_metrics['sum'],
    bins=[0, 1000, 5000, float('inf')],
    labels=['Low', 'Medium', 'High']
)

# Trend analysis
X = np.arange(len(monthly_sales)).reshape(-1, 1)
y = monthly_sales['sum'].values
model = LinearRegression().fit(X, y)
trend_slope = model.coef_[0]

result = {
    "monthly_trends": monthly_sales.to_dict(),
    "growth_rate": float(growth_rate),
    "customer_segments": customer_metrics['segment'].value_counts().to_dict(),
    "trend_slope": float(trend_slope)
}

print(json.dumps(result))
"""
        }],
        parameters={"cleaned_data": "{{preprocess_step.output.cleaned_data}}"}
    ),
    depends_on=preprocess_step,
    key="statistical_analysis"
)

# Step 4: Generate visualizations and reports
visualization_step = sales_analyzer.step(
    codeexec.execute(
        code_files=[{
            "filename": "create_visualizations.py",
            "content": """
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import json

# Create comprehensive visualizations
plt.figure(figsize=(15, 10))

# Revenue trend
plt.subplot(2, 2, 1)
monthly_data = {{stats_step.output.monthly_trends}}
plt.plot(range(len(monthly_data['sum'])), list(monthly_data['sum'].values()))
plt.title('Revenue Trend Over Time')
plt.xlabel('Month')
plt.ylabel('Revenue')

# Customer segments
plt.subplot(2, 2, 2)
segments = {{stats_step.output.customer_segments}}
plt.pie(segments.values(), labels=segments.keys(), autopct='%1.1f%%')
plt.title('Customer Segments')

plt.tight_layout()
plt.savefig('sales_analysis_dashboard.png', dpi=300, bbox_inches='tight')

result = {
    "dashboard_created": True,
    "charts_generated": 2,
    "file_path": "sales_analysis_dashboard.png"
}

print(json.dumps(result))
"""
        }]
    ),
    depends_on=stats_step,
    key="visualizations"
)

# Handle successful preprocessing
@preprocess_step.when(IsSuccess())
def store_clean_data(result):
    """Store cleaned data for future use."""
    return memory.store(memory={
        "content": result.output.cleaned_data,
        "description": "Cleaned sales data with quality metrics",
        "type": "cleaned_sales_data",
        "tags": ["sales", "cleaned", "preprocessed"],
        "extra": result.output.data_quality
    })

# Handle high-quality analysis results
@analysis_step.when(IsSuccess())
def store_insights(result):
    """Store business insights."""
    return memory.store(memory={
        "content": result.output.insights,
        "description": "Sales analysis insights and recommendations",
        "type": "business_insights",
        "tags": ["sales", "insights", "analysis"],
        "extra": {
            "recommendations": result.output.recommendations,
            "key_findings": result.output.key_findings
        }
    })

# Handle statistical analysis completion
@stats_step.when(IsSuccess() & GreaterThan("growth_rate", 0))
def celebrate_growth(result):
    """Highlight positive growth trends."""
    return utils.send_status(
        status="positive_growth",
        message=f"Excellent! Sales showing {result.output.growth_rate:.2%} growth rate",
        priority="high"
    )

# Handle error cases
@preprocess_step.when(IsError())
def handle_data_error(error):
    """Handle data preprocessing errors."""
    return utils.send_status(
        status="data_error",
        message=f"Data preprocessing failed: {error.message}",
        error_details=error.details
    )

@analysis_step.when(IsError())
def handle_analysis_error(error):
    """Handle analysis errors."""
    return utils.send_status(
        status="analysis_error",
        message=f"Analysis failed: {error.message}",
        suggestion="Check data quality and try again"
    )
```

<CardGroup cols={2}>
  <Card title="Key Features" icon="star">
    - Automated data quality validation - Advanced statistical analysis -
    Customer segmentation - Trend and seasonality detection - Interactive
    visualizations - Actionable recommendations
  </Card>
  <Card title="Business Value" icon="chart-line">
    - Identify revenue opportunities - Understand customer behavior - Optimize
    sales strategies - Predict future performance - Monitor KPIs automatically -
    Data-driven decision making
  </Card>
</CardGroup>

## Financial Data Processor

Process financial statements and calculate key ratios:

```python
financial_analyzer = Agent(
    name="financial_analyzer",
    description="Analyzes financial statements and calculates key ratios"
)

# Step 1: Parse financial statements
parse_financials = financial_analyzer.step(
    llm.message(
        model="claude-sonnet-4",
        query="Extract financial data from: {{financial_statements}}",
        response_format={
            "Type": "json_schema",
            "Schema": {
                "schema": {
                    "type": "object",
                    "properties": {
                        "balance_sheet": {
                            "type": "object",
                            "properties": {
                                "total_assets": {"type": "number"},
                                "total_liabilities": {"type": "number"},
                                "shareholders_equity": {"type": "number"}
                            }
                        },
                        "income_statement": {
                            "type": "object",
                            "properties": {
                                "revenue": {"type": "number"},
                                "gross_profit": {"type": "number"},
                                "net_income": {"type": "number"}
                            }
                        },
                        "cash_flow": {
                            "type": "object",
                            "properties": {
                                "operating_cash_flow": {"type": "number"},
                                "investing_cash_flow": {"type": "number"},
                                "financing_cash_flow": {"type": "number"}
                            }
                        }
                    }
                }
            }
        }
    )
)

# Step 2: Calculate financial ratios
calculate_ratios = financial_analyzer.step(
    codeexec.execute(
        code="""
# Extract financial data
balance_sheet = {{parse.output.balance_sheet}}
income_statement = {{parse.output.income_statement}}
cash_flow = {{parse.output.cash_flow}}

# Liquidity ratios
current_ratio = balance_sheet.get('current_assets', 0) / balance_sheet.get('current_liabilities', 1)
quick_ratio = (balance_sheet.get('current_assets', 0) - balance_sheet.get('inventory', 0)) / balance_sheet.get('current_liabilities', 1)

# Profitability ratios
gross_margin = income_statement.get('gross_profit', 0) / income_statement.get('revenue', 1)
net_margin = income_statement.get('net_income', 0) / income_statement.get('revenue', 1)
roe = income_statement.get('net_income', 0) / balance_sheet.get('shareholders_equity', 1)

# Efficiency ratios
asset_turnover = income_statement.get('revenue', 0) / balance_sheet.get('total_assets', 1)
debt_to_equity = balance_sheet.get('total_liabilities', 0) / balance_sheet.get('shareholders_equity', 1)

# Cash flow ratios
operating_margin = cash_flow.get('operating_cash_flow', 0) / income_statement.get('revenue', 1)

return {
    "liquidity": {
        "current_ratio": round(current_ratio, 2),
        "quick_ratio": round(quick_ratio, 2)
    },
    "profitability": {
        "gross_margin": round(gross_margin * 100, 2),
        "net_margin": round(net_margin * 100, 2),
        "roe": round(roe * 100, 2)
    },
    "efficiency": {
        "asset_turnover": round(asset_turnover, 2),
        "debt_to_equity": round(debt_to_equity, 2)
    },
    "cash_flow": {
        "operating_margin": round(operating_margin * 100, 2)
    }
}
"""
    ),
    depends_on=parse_financials,
    key="ratios"
)

# Step 3: Financial health assessment
assessment_step = financial_analyzer.step(
    llm.message(
        model="claude-sonnet-4",
        query="Assess financial health based on ratios: {{ratios.output.content}}",
        system_prompt="You are a financial analyst. Provide comprehensive assessment of financial health."
    ),
    depends_on=calculate_ratios,
    key="assess"
)

agents.append(financial_analyzer)
```

## Market Research Analyzer

Analyze market trends and competitive landscape:

```python
market_researcher = Agent(
    name="market_research_analyzer",
    description="Analyzes market trends and competitive intelligence"
)

# Step 1: Gather market data
research_step = market_researcher.step(
    llm.message(
        model="claude-sonnet-4",
        query="Research market trends for {{industry}} in {{region}}",
        tools=[
            Tool(
                name="web_search",
                description="Search for market information",
                actiontype="websearch.search",
                parameters={
                    "query": "{{industry}} market trends {{region}} 2024",
                    "num_results": 10
                }
            ),
            Tool(
                name="parse_reports",
                description="Parse market research reports",
                actiontype="webparser.parse",
                parameters={"url": "{{url}}"}
            )
        ]
    )
)

# Step 2: Competitive analysis
competitive_analysis = market_researcher.step(
    llm.message(
        model="claude-sonnet-4",
        query="Analyze competitive landscape: {{research.output.content}}",
        response_format={
            "Type": "json_schema",
            "Schema": {
                "schema": {
                    "type": "object",
                    "properties": {
                        "market_size": {"type": "string"},
                        "growth_rate": {"type": "string"},
                        "key_players": {"type": "array"},
                        "market_trends": {"type": "array"},
                        "opportunities": {"type": "array"},
                        "threats": {"type": "array"}
                    }
                }
            }
        }
    ),
    depends_on=research_step,
    key="competitive"
)

# Step 3: SWOT analysis
swot_analysis = market_researcher.step(
    llm.message(
        model="claude-sonnet-4",
        query="Generate SWOT analysis for {{company}} in {{industry}}",
        system_prompt="Create comprehensive SWOT analysis based on market research."
    ),
    depends_on=competitive_analysis,
    key="swot"
)

agents.append(market_researcher)
```

## Performance Analytics Dashboard

Create automated performance dashboards:

```python
dashboard_generator = Agent(
    name="performance_dashboard",
    description="Generates automated performance analytics dashboards"
)

# Step 1: Data aggregation
aggregate_data = dashboard_generator.step(
    codeexec.execute(
        code="""
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

# Aggregate data from multiple sources
sources = {{data_sources}}
aggregated_data = {}

for source_name, source_data in sources.items():
    df = pd.DataFrame(source_data)

    # Calculate key metrics
    metrics = {
        'total_volume': df['volume'].sum() if 'volume' in df.columns else 0,
        'average_value': df['value'].mean() if 'value' in df.columns else 0,
        'growth_rate': calculate_growth_rate(df),
        'trend_direction': determine_trend(df),
        'data_points': len(df)
    }

    aggregated_data[source_name] = metrics

# Overall performance calculation
overall_performance = {
    'total_sources': len(sources),
    'combined_volume': sum(data['total_volume'] for data in aggregated_data.values()),
    'avg_growth_rate': np.mean([data['growth_rate'] for data in aggregated_data.values()]),
    'data_freshness': calculate_data_freshness(sources)
}

return {
    'source_metrics': aggregated_data,
    'overall_performance': overall_performance,
    'last_updated': datetime.now().isoformat()
}
"""
    )
)

# Step 2: Generate visualizations
create_dashboard = dashboard_generator.step(
    codeexec.execute(
        code="""
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px

# Create interactive dashboard
fig = make_subplots(
    rows=3, cols=2,
    subplot_titles=[
        'Performance Overview', 'Growth Trends',
        'Source Comparison', 'Volume Analysis',
        'Trend Indicators', 'Data Quality Metrics'
    ],
    specs=[
        [{"type": "indicator"}, {"type": "scatter"}],
        [{"type": "bar"}, {"type": "scatter"}],
        [{"type": "pie"}, {"type": "table"}]
    ]
)

metrics = {{aggregate.output.source_metrics}}
overall = {{aggregate.output.overall_performance}}

# Performance indicators
fig.add_trace(
    go.Indicator(
        mode="gauge+number+delta",
        value=overall['avg_growth_rate'],
        title={'text': "Average Growth Rate"},
        gauge={'axis': {'range': [-20, 50]}}
    ),
    row=1, col=1
)

# Save dashboard
fig.write_html('performance_dashboard.html')
fig.write_image('performance_dashboard.png')

return {
    'dashboard_created': True,
    'html_path': 'performance_dashboard.html',
    'image_path': 'performance_dashboard.png',
    'charts_count': 6
}
"""
    ),
    depends_on=aggregate_data,
    key="dashboard"
)

# Step 3: Generate insights and alerts
insights_step = dashboard_generator.step(
    llm.message(
        model="claude-sonnet-4",
        query="Generate insights and alerts from performance data: {{aggregate.output.content}}",
        response_format={
            "Type": "json_schema",
            "Schema": {
                "schema": {
                    "type": "object",
                    "properties": {
                        "key_insights": {"type": "array"},
                        "alerts": {"type": "array"},
                        "recommendations": {"type": "array"},
                        "next_actions": {"type": "array"}
                    }
                }
            }
        }
    ),
    depends_on=create_dashboard,
    key="insights"
)

agents.append(dashboard_generator)
```

## Advanced Analytics Examples

<Tabs>
  <Tab title="Predictive Analytics">
    ```python
    # Predictive sales forecasting
    forecast_step = predictor_agent.step(
        codeexec.execute(
            code="""
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.metrics import mean_absolute_error
    import pandas as pd

    # Prepare time series data
    data = prepare_time_series_data({{historical_data}})

    # Feature engineering
    features = create_features(data)
    X, y = prepare_ml_data(features)

    # Train model
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X, y)

    # Generate predictions
    future_features = create_future_features({{forecast_periods}})
    predictions = model.predict(future_features)

    # Calculate confidence intervals
    confidence_intervals = calculate_confidence_intervals(model, future_features)

    return {
        'predictions': predictions.tolist(),
        'confidence_intervals': confidence_intervals,
        'model_accuracy': calculate_accuracy_metrics(model, X, y),
        'feature_importance': dict(zip(features.columns, model.feature_importances_))
    }
    """
        ),
        key="forecast"
    )
    ```

  </Tab>
  <Tab title="Anomaly Detection">
    ```python
    # Detect anomalies in data
    anomaly_detection = anomaly_agent.step(
        codeexec.execute(
            code="""
    from sklearn.ensemble import IsolationForest
    from sklearn.preprocessing import StandardScaler
    import numpy as np

    # Prepare data
    data = np.array({{time_series_data}})
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(data.reshape(-1, 1))

    # Anomaly detection
    detector = IsolationForest(contamination=0.1, random_state=42)
    anomalies = detector.fit_predict(scaled_data)

    # Identify anomalous points
    anomaly_indices = np.where(anomalies == -1)[0]
    anomaly_values = data[anomaly_indices]

    # Calculate severity scores
    severity_scores = detector.decision_function(scaled_data)

    return {
        'anomaly_count': len(anomaly_indices),
        'anomaly_indices': anomaly_indices.tolist(),
        'anomaly_values': anomaly_values.tolist(),
        'severity_scores': severity_scores.tolist(),
        'anomaly_percentage': (len(anomaly_indices) / len(data)) * 100
    }
    """
        ),
        key="detect_anomalies"
    )
    ```
