---
title: "Vercel AI SDK Integration"
description: "Use Erdo's MCP tools with Vercel AI SDK for rich data analysis in your chat UI"
---

# Vercel AI SDK Integration

Connect Erdo's MCP tools to any LLM (Claude, GPT, etc.) using [Vercel AI SDK](https://sdk.vercel.ai). Your users ask data questions in a chat UI, the LLM calls Erdo tools to analyze data, and you render rich charts and tables with `ErdoToolResult`.

```
Browser (useChat) → Your API Route → LLM → Erdo MCP Server → Tool Results
                                                                    ↓
Browser ← streamed response ← LLM + tool results ←────────────────┘
```

## Installation

<CodeGroup>

```bash npm
npm install @erdoai/ui ai @ai-sdk/react @ai-sdk/mcp @ai-sdk/anthropic
```

```bash yarn
yarn add @erdoai/ui ai @ai-sdk/react @ai-sdk/mcp @ai-sdk/anthropic
```

```bash pnpm
pnpm add @erdoai/ui ai @ai-sdk/react @ai-sdk/mcp @ai-sdk/anthropic
```

</CodeGroup>

Replace `@ai-sdk/anthropic` with your preferred model provider (`@ai-sdk/openai`, `@ai-sdk/google`, etc.).

## Quick Start

### 1. Create an API Route (Server)

The API route connects to Erdo's MCP server, discovers tools, and passes them to the LLM:

```typescript
// app/api/chat/route.ts
import { createMCPClient } from '@ai-sdk/mcp';
import { streamText, convertToModelMessages, type ToolSet } from 'ai';
import { anthropic } from '@ai-sdk/anthropic';

export async function POST(req: Request) {
  const { messages } = await req.json();

  // Connect to Erdo's MCP server
  const mcpClient = await createMCPClient({
    transport: {
      type: 'http',
      url: `${process.env.ERDO_ENDPOINT || 'https://api.erdo.ai'}/mcp`,
      headers: {
        Authorization: `Bearer ${process.env.ERDO_AUTH_TOKEN}`,
      },
    },
  });

  // Discover Erdo tools (list_datasets, ask_data_question, query_data, etc.)
  const tools = await mcpClient.tools();

  // Convert UI messages (from useChat) to model messages (for streamText)
  const modelMessages = await convertToModelMessages(messages);

  const result = streamText({
    model: anthropic('claude-sonnet-4-20250514'),
    messages: modelMessages,
    tools: tools as ToolSet,
    system: 'You are a helpful data analyst. Use the Erdo tools to answer data questions.',
    onFinish: async () => {
      await mcpClient.close();
    },
  });

  return result.toUIMessageStreamResponse();
}
```

<Warning>
Never expose your `ERDO_AUTH_TOKEN` to the browser. The MCP connection happens server-side only.
</Warning>

### 2. Render Results (Client)

Use `isErdoTool` and `ErdoToolResult` to render rich charts and tables from tool results:

```tsx
// app/chat/page.tsx
'use client';

import { useChat } from '@ai-sdk/react';
import { DefaultChatTransport, isToolUIPart } from 'ai';
import { ErdoProvider, ErdoToolResult, isErdoTool } from '@erdoai/ui';

export default function ChatPage() {
  const { messages, sendMessage, status } = useChat({
    transport: new DefaultChatTransport({ api: '/api/chat' }),
  });

  const isStreaming = status === 'streaming' || status === 'submitted';

  return (
    <ErdoProvider config={{ baseUrl: 'https://api.erdo.ai' }}>
      <div>
        {messages.map((message) => (
          <div key={message.id}>
            {message.parts.map((part, i) => {
              // Erdo tool results → rich charts, tables, markdown
              if (isToolUIPart(part) && isErdoTool(part)) {
                return <ErdoToolResult key={part.toolCallId} part={part} />;
              }

              // Text parts
              if (part.type === 'text' && part.text) {
                return <p key={i}>{part.text}</p>;
              }

              return null;
            })}
          </div>
        ))}

        <form onSubmit={(e) => {
          e.preventDefault();
          const input = e.currentTarget.querySelector('input') as HTMLInputElement;
          if (input.value.trim()) {
            sendMessage({ text: input.value.trim() });
            input.value = '';
          }
        }}>
          <input
            placeholder="Ask a data question..."
            disabled={isStreaming}
          />
          <button type="submit" disabled={isStreaming}>Send</button>
        </form>
      </div>
    </ErdoProvider>
  );
}
```

### 3. Set Environment Variables

```bash
# .env.local
ERDO_AUTH_TOKEN=your-api-key        # Server-side only
ERDO_ENDPOINT=https://api.erdo.ai   # Optional, defaults to https://api.erdo.ai
```

That's it. The LLM will automatically use Erdo tools like `erdo_list_datasets`, `erdo_ask_data_question`, and `erdo_query_data` based on user questions.

## Bridge Pattern (Alternative)

If you're already using `@erdoai/server`, you can use `client.getTools()` instead of creating an MCP client directly:

```typescript
// app/api/chat/route.ts
import { ErdoClient } from '@erdoai/server';
import { streamText, convertToModelMessages, type ToolSet } from 'ai';
import { anthropic } from '@ai-sdk/anthropic';

const erdoClient = new ErdoClient({
  authToken: process.env.ERDO_AUTH_TOKEN,
});

export async function POST(req: Request) {
  const { messages } = await req.json();

  // getTools() connects to Erdo's MCP server and returns AI SDK-compatible tools
  const { tools, close } = await erdoClient.getTools();

  const modelMessages = await convertToModelMessages(messages);

  const result = streamText({
    model: anthropic('claude-sonnet-4-20250514'),
    messages: modelMessages,
    tools: tools as ToolSet,
    system: 'You are a helpful data analyst. Use the Erdo tools to answer data questions.',
    onFinish: close,
  });

  return result.toUIMessageStreamResponse();
}
```

<Info>
`getTools()` requires `@ai-sdk/mcp` as a peer dependency. Install it with `npm install @ai-sdk/mcp`.
</Info>

The client-side rendering code is the same — use `isErdoTool` + `ErdoToolResult` as shown above.

## Component Reference

### `isErdoTool(part)`

Checks if an AI SDK message part is an Erdo tool call/result.

```typescript
import { isErdoTool } from '@erdoai/ui';

// Works with both static and dynamic tool parts:
// - Static: { type: 'tool-erdo_list_datasets', ... }
// - Dynamic (MCP): { type: 'dynamic-tool', toolName: 'erdo_list_datasets', ... }
isErdoTool(part) // → true for any erdo_* tool
```

### `ErdoToolResult`

Renders an Erdo tool result with appropriate UI based on the tool type:

- **UI tools** (`erdo_ask_data_question`, `erdo_query_data`): Charts, tables, and markdown
- **Data tools** (`erdo_list_datasets`, `erdo_get_dataset_schema`, etc.): Formatted JSON
- **Loading states**: Spinner with tool name
- **Errors**: Error message display

```tsx
import { ErdoToolResult, type ErdoToolResultProps } from '@erdoai/ui';

<ErdoToolResult
  part={part}              // AI SDK tool part from message.parts
  className="my-4"        // Optional styling
  invocationId="inv-123"  // Optional, for chart/table data fetching
  onSuggestionClick={fn}  // Optional callback for follow-up suggestions
/>
```

### `getErdoToolName(part)`

Extracts the Erdo tool name from an AI SDK message part, or returns `undefined` if it's not an Erdo tool.

```typescript
import { getErdoToolName } from '@erdoai/ui';

getErdoToolName(part) // → 'erdo_list_datasets' | 'erdo_ask_data_question' | ...
```

### `ErdoProvider`

Required for chart and table rendering. Provides the data fetching context so charts can load dataset contents.

```tsx
import { ErdoProvider } from '@erdoai/ui';

<ErdoProvider
  config={{
    baseUrl: 'https://api.erdo.ai',
    token: scopedToken,  // Optional: scoped token for client-side auth
  }}
>
  {children}
</ErdoProvider>
```

<Info>
If you don't need chart/table rendering (only text and JSON results), `ErdoProvider` is optional.
</Info>

## Authentication

### Server-Side (MCP Connection)

The MCP connection uses your API key, which stays server-side:

```bash
ERDO_AUTH_TOKEN=your-api-key
```

### Client-Side (ErdoProvider)

For chart and table rendering, `ErdoProvider` needs to fetch dataset contents. You have two options:

1. **No auth needed** if charts don't load external data (simple results work without auth)
2. **Scoped token** for full chart/table support — create a short-lived token from your backend:

```typescript
// Server: create a scoped token
const { token } = await erdoClient.createToken({
  botKeys: ['my-org.data-analyst'],
  datasetIds: ['dataset-uuid'],
  expiresInSeconds: 3600,
});

// Client: pass to ErdoProvider
<ErdoProvider config={{ baseUrl: 'https://api.erdo.ai', token }}>
```

See [Scoped Tokens](/ts-sdk/integration#scoped-tokens-b2b2c) for full details.

## Available Tools

When connected via MCP, the LLM can use these Erdo tools:

| Tool | Description |
|------|-------------|
| `erdo_list_datasets` | List datasets with name, type, and description |
| `erdo_get_dataset_schema` | Get column names, types, and statistics |
| `erdo_gather_dataset_context` | Get context for multiple datasets at once |
| `erdo_search_data` | Search across datasets with natural language |
| `erdo_ask_data_question` | AI analysis with charts, tables, and text |
| `erdo_query_data` | Natural language SQL queries |
| `erdo_run_query` | Execute raw SQL queries |

See [MCP Server](/mcp/overview#available-tools) for full parameter details.

## Example App

A complete working example is available in the SDK repository:

```bash
git clone https://github.com/erdoai/erdo-ts-sdk
cd erdo-ts-sdk/examples/nextjs-vercel-ai
```

It demonstrates three integration patterns:
- **MCP Pattern** (`/mcp`) — MCP tools via AI SDK with `ErdoToolResult` rendering
- **Token Pattern** (`/`) — Direct streaming with ephemeral tokens
- **Proxy Pattern** (`/proxy`) — Server-proxied SSE streaming
