---
title: "Testing Overview"
description: "Comprehensive testing strategies for Erdo agents"
---

# Testing Overview

Erdo provides two complementary testing approaches that enable comprehensive validation while minimizing API costs: unit testing for logic validation and integration testing for end-to-end verification.

## Testing Philosophy

Agent systems present unique testing challenges:
- **Complexity**: Multiple execution paths through conditional logic and result handlers
- **State Validation**: Ensuring data flows correctly through templates and steps
- **Coverage**: Identifying and testing all possible paths through the agent
- **Integration Costs**: LLM API calls in integration tests can accumulate

Erdo addresses these through:
1. **Declarative structure** that enables automatic path enumeration
2. **Static analysis** that validates templates before execution
3. **Intelligent caching** for cost-effective integration testing
4. **Parallel execution** for rapid feedback

## Test Types Comparison

Erdo provides two distinct test types, each serving different purposes:

| Aspect | Unit Tests (`erdo test`) | Integration Tests (`erdo agent-test`) |
|--------|--------------------------|---------------------------------------|
| **What it does** | Validates agent structure & logic locally | Executes agents with real backend/LLM calls |
| **Backend calls** | âŒ None - pure static analysis | âœ… Yes - actual execution |
| **Speed** | âš¡ 2-5 seconds for full coverage | ğŸ¢ Depends on LLM response time (or fast with replay cache) |
| **Cost** | ğŸ’° Free - no API calls | ğŸ’° First run costs API fees, replay mode caches for free reruns |
| **Coverage** | ğŸ” All execution paths (2^n for n conditions) | ğŸ¯ Specific test scenarios you write |
| **Use case** | Validate logic, catch template errors, rapid development | End-to-end validation, integration verification |
| **When to run** | Every save/commit - instant feedback | Before deployment, CI/CD, regression testing |
| **Test files** | Any agent Python file | Functions with `agent_test_*` prefix |
| **Command** | `erdo test my_agent.py` | `erdo agent-test tests/test_my_agent.py` |

<Info>
**Quick Decision Guide:**
- **Use Unit Tests** when you want fast feedback on agent structure, template syntax, and execution paths
- **Use Integration Tests** when you need to validate actual LLM behavior and end-to-end functionality
- **Use Both** for comprehensive coverage: unit tests catch structure issues instantly, integration tests validate real behavior
</Info>

## Unit Testing

<Note>
**Local Validation Only**: Unit tests run entirely on your machine with NO backend or LLM calls. They're free, fast, and perfect for rapid development iteration. See the [Unit Testing Guide](/testing/unit-tests) for complete details.
</Note>

### Overview

Unit tests validate agent structure and logic through static analysis and path enumeration:

```bash
erdo test my_agent.py
```

### What Gets Tested

**Execution Path Enumeration**
Because Erdo agents are declarative, the CLI can automatically identify all execution paths:
- All conditional branches (success/error handlers)
- ITERATE_OVER loops with different data structures
- Step dependencies and ordering
- Handler combinations

**Template Validation**
Templates are validated against actual data structures:
- State field availability and access patterns
- Type compatibility
- Parameter hydration with test data
- Missing key detection

**State Management**
State flow is simulated through execution:
- Step output accumulation
- Data transformations
- Context availability at each step

### Example Output

```
ğŸ” Testing Python agents in my_agent.py...
ğŸ¤– Found 1 agents to test:
  â€¢ data_analyzer: Analyzes data and generates insights

ğŸ“Š Generating execution paths...

âœ… Path 1: analysis â†’ store_high_confidence â†’ send_notification
âœ… Path 2: analysis â†’ request_review â†’ notify_team
âœ… Path 3: analysis [error] â†’ retry â†’ notify_failure

âœ… All 12 execution paths tested successfully!
```

### Running Unit Tests

```bash
# Test all agents in current directory
erdo test

# Test specific agent
erdo test agents/data_analyzer.py

# Provide custom test data
erdo test agents/data_analyzer.py --data test_scenarios.json

# Watch mode for development
erdo test agents/data_analyzer.py --watch
```

### Test Data Generation

Erdo automatically generates test data from your parameter definitions:

```python
agent = Agent(
    name="analyzer",
    parameter_definitions=[
        ParameterDefinition(
            name="Query",
            key="query",
            type=ParameterType.STRING,
            is_required=True
        )
    ]
)

# erdo test will generate: {"query": "sample_query"}
```

You can also provide custom test data:

```json
// test_data.json
{
  "query": "Analyze Q4 sales trends",
  "dataset": {
    "id": "sales_2024_q4",
    "name": "Q4 Sales Data",
    "type": "file"
  }
}
```

```bash
erdo test my_agent.py --data test_data.json
```

## Integration Testing

<Note>
**Live Agent Execution**: Integration tests execute your agents with real backend and LLM calls. Use replay mode to cache responses and minimize API costs. See the [Integration Testing Guide](/testing/integration-tests) for complete details on modes and best practices.
</Note>

### Overview

Integration tests execute agents with real LLM calls. They support three modes (live, replay, manual) for different testing needs:

```python
from erdo import invoke
from erdo.test import text_contains

def agent_test_sales_analysis():
    """Test sales analysis with actual LLM execution."""
    response = invoke(
        "data-analyst",
        messages=[{"role": "user", "content": "Analyze Q4 sales"}],
        datasets=["sales_2024_q4"],
        mode="replay"
    )
    
    assert response.success
    assert text_contains(str(response.result), "revenue")
```

### Replay Mode

Replay mode intelligently caches LLM responses:

**First Execution**
1. Executes agent with real LLM calls
2. Caches responses locally
3. Test runs with actual API behavior

**Subsequent Executions**
1. Returns cached responses without API calls
2. Tests run in milliseconds instead of seconds
3. Identical behavior to first execution

**Cache Invalidation**
- Automatic when agent definition changes
- Manual refresh with `--refresh` flag
- Per-test cache isolation

### Test Modes

<Tabs>
  <Tab title="Replay Mode (Recommended)">
    ```python
    response = invoke(
        "my-agent",
        messages=[{"role": "user", "content": "test"}],
        mode="replay"
    )
    ```

    **Use for:**
    - Most integration tests
    - CI/CD pipelines
    - Regression testing
    - Rapid development iteration

    **Behavior:**
    - First run: Real API calls, response cached
    - Subsequent runs: Cached response, no API calls
    - Deterministic test results
  </Tab>
  <Tab title="Live Mode">
    ```python
    response = invoke(
        "my-agent",
        messages=[{"role": "user", "content": "test"}],
        mode="live"
    )
    ```

    **Use for:**
    - Integration tests requiring fresh data
    - Testing with latest model behavior
    - Validating cache integrity

    **Behavior:**
    - Always makes real API calls
    - Non-deterministic results
    - API costs per execution
  </Tab>
  <Tab title="Manual Mode">
    ```python
    response = invoke(
        "my-agent",
        messages=[{"role": "user", "content": "test"}],
        mode="manual",
        manual_mocks={
            "llm.message": {
                "status": "success",
                "output": {"content": "Mocked response"}
            }
        }
    )
    ```

    **Use for:**
    - Deterministic test scenarios
    - Testing error handling
    - Offline development
    - Fast unit-style tests

    **Behavior:**
    - No API calls
    - Developer-controlled responses
    - Predictable outcomes
  </Tab>
</Tabs>

### Running Integration Tests

```bash
# Run all tests in parallel
erdo agent-test tests/test_my_agent.py

# Verbose output for debugging
erdo agent-test tests/test_my_agent.py --verbose

# Refresh cached responses
erdo agent-test tests/test_my_agent.py --refresh

# Control parallelism
erdo agent-test tests/test_my_agent.py -j 8
```

### Test Helpers

```python
from erdo.test import (
    text_contains,
    text_equals,
    text_matches,
    json_path_equals,
    json_path_exists
)

def agent_test_analysis():
    response = invoke("analyst", messages=[...], mode="replay")
    
    # Text assertions
    result = str(response.result)
    assert text_contains(result, "insights", case_sensitive=False)
    assert text_matches(result, r"\d+ recommendations")
    
    # JSON assertions
    assert json_path_exists(response.result, "analysis.summary")
    assert json_path_equals(response.result, "analysis.confidence", 0.95)
```

## Test Organization

### File Structure

```
my_project/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ data_analyzer.py
â”‚   â””â”€â”€ report_generator.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_data_analyzer.py
â”‚   â””â”€â”€ test_report_generator.py
â””â”€â”€ test_data/
    â”œâ”€â”€ scenarios.json
    â””â”€â”€ fixtures.json
```

### Naming Convention

```python
# Integration tests: agent_test_* prefix
def agent_test_csv_analysis():
    """Test CSV data analysis."""
    pass

def agent_test_error_handling():
    """Test error handling behavior."""
    pass

# Helper functions: no prefix requirement
def load_test_data():
    """Load test data fixture."""
    pass
```

## Best Practices

### Test Coverage Strategy

1. **Unit Tests**: Validate all execution paths
2. **Integration Tests**: Test critical user journeys
3. **Edge Cases**: Handle error conditions and boundary cases
4. **Regression Tests**: Prevent known issues from recurring

### Cost-Effective Testing

```python
# Good: Most tests use replay mode
def agent_test_standard_query():
    response = invoke("agent", messages=[...], mode="replay")
    assert response.success

# Strategic: Use live mode sparingly for critical paths
def agent_test_production_validation():
    response = invoke("agent", messages=[...], mode="live")
    assert response.success

# Efficient: Use manual mode for deterministic scenarios
def agent_test_error_handling():
    response = invoke(
        "agent",
        messages=[...],
        mode="manual",
        manual_mocks={"llm.message": {"status": "error", "error": "Test error"}}
    )
    assert not response.success
```

### Continuous Integration

```yaml
# .github/workflows/test.yml
name: Test Agents

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      
      - name: Install Erdo CLI
        run: |
          brew install erdoai/tap/erdo
          erdo login --token ${{ secrets.ERDO_TOKEN }}
      
      - name: Unit Tests
        run: erdo test agents/
      
      - name: Integration Tests
        run: erdo agent-test tests/
```

## Performance Characteristics

### Unit Tests
- **Duration**: 2-5 seconds for comprehensive path coverage
- **Cost**: No API calls
- **Coverage**: All execution paths automatically tested

### Integration Tests with Replay Mode
- **First Run**: Standard API call duration + caching overhead
- **Subsequent Runs**: Milliseconds (cached responses)
- **Cost**: API costs only on first run or cache refresh
- **Parallelism**: 10x faster with parallel execution

### Integration Tests with Live Mode
- **Duration**: Standard API call duration per test
- **Cost**: API costs per execution
- **Use Case**: Strategic validation only

## Next Steps

<CardGroup cols={2}>
  <Card title="Unit Testing Guide" icon="flask" href="/testing/unit-tests">
    Learn about comprehensive local validation
  </Card>
  <Card title="Integration Testing" icon="plug" href="/testing/integration-tests">
    Master replay mode and test strategies
  </Card>
  <Card title="CLI Reference" icon="terminal" href="/cli/commands">
    Complete testing command reference
  </Card>
  <Card title="Examples" icon="code" href="/examples/business-automation">
    Real-world agent examples
  </Card>
</CardGroup>

